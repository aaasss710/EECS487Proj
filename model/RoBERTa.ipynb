{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    716\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m                     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 8 at dim 1 (got 7)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20928\\450750024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Prepare the input text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"This is an example sentence.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"this is another example sentence\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mencoded_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Feed the input into the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2607\u001b[0m                 )\n\u001b[0;32m   2608\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2609\u001b[1;33m             return self.batch_encode_plus(\n\u001b[0m\u001b[0;32m   2610\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2611\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2798\u001b[0m         )\n\u001b[0;32m   2799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         batch_outputs = self._batch_prepare_for_model(\n\u001b[0m\u001b[0;32m    738\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[1;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[0;32m    815\u001b[0m         )\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m         \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\roger\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    731\u001b[0m                         \u001b[1;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                     )\n\u001b[1;32m--> 733\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    734\u001b[0m                     \u001b[1;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m                     \u001b[1;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "try_model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the input text\n",
    "text = [\"This is an example sentence.\", \"this is another example sentence\"]\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Feed the input into the model\n",
    "output = try_model(**encoded_input)\n",
    "\n",
    "# Extract the CLS token embedding\n",
    "cls_embedding = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(cls_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from load_wiki_dataset import wikiData\n",
    "from RoBERTa import CustomRobertaModel\n",
    "from losses import align_loss, uniform_loss\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')\n",
    "    parser.add_argument('--data_path', type=str, default='../data/wiki1m_for_simcse.txt', help='Path to the dataset')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def train_wiki(args,model):\n",
    "    # Load dataset\n",
    "    with open(args.data_path, 'r', encoding='UTF-8') as f:\n",
    "        input_text = f.readlines()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # ADD THIS LINE\n",
    "\n",
    "    wiki = wikiData(input_text, tokenizer)  # PASS tokenizer TO wikiData\n",
    "    train_params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    trainloader = DataLoader(wiki, **train_params)\n",
    "    \n",
    "    # Initialize model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    for epoch in range(args.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}  # move batch to device\n",
    "                \n",
    "                loss, _ = model(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                # print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    torch.save(model.state_dict(), f'our_loss_{epoch + 1}wiki_model.pth')\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "raw_model= CustomRobertaModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125000/125000 [5:39:37<00:00,  6.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.7516470831336677\n"
     ]
    }
   ],
   "source": [
    "simcse_1wiki_model = train_wiki(args, raw_model)\n",
    "    # Save the trained model if needed\n",
    "simcse_1wiki_model.save_pretrained('./simcse_1wiki_model')\n",
    "torch.save(simcse_1wiki_model.state_dict(), 'simcse_1wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RoBERTa import CustomRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model2 = CustomRobertaModel()\n",
    "wiki_model2.load_state_dict(torch.load('1wiki.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./trained_model_wiki were not used when initializing RobertaModel: ['roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.value.bias', 'roberta.roberta.encoder.layer.4.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.attention.self.query.weight', 'roberta.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.output.dense.weight', 'roberta.roberta.encoder.layer.8.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.roberta.encoder.layer.11.output.dense.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.key.weight', 'roberta.roberta.encoder.layer.1.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.attention.self.key.bias', 'roberta.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.query.weight', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.7.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.query.weight', 'roberta.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.output.dense.weight', 'roberta.roberta.encoder.layer.9.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.5.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.4.attention.self.query.bias', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.10.attention.self.query.weight', 'roberta.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.8.attention.self.value.bias', 'roberta.roberta.encoder.layer.8.attention.output.dense.weight', 'roberta_m.roberta.embeddings.position_embeddings.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.query.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.query.bias', 'roberta.roberta.encoder.layer.11.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.2.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.roberta.embeddings.LayerNorm.weight', 'roberta.roberta.encoder.layer.8.attention.self.query.bias', 'roberta.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.2.output.dense.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.attention.self.key.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.3.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.embeddings.token_type_embeddings.weight', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.11.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.3.output.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.1.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.8.output.dense.weight', 'roberta_m.roberta.encoder.layer.11.attention.self.value.bias', 'roberta.roberta.encoder.layer.3.attention.self.query.bias', 'roberta.roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.roberta.encoder.layer.3.output.dense.bias', 'roberta.roberta.encoder.layer.11.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.7.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.key.weight', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.output.dense.bias', 'roberta_m.roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.value.weight', 'roberta.roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.5.attention.self.value.weight', 'roberta.roberta.encoder.layer.6.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.10.output.dense.weight', 'roberta_m.roberta.encoder.layer.7.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.value.bias', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.value.weight', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.query.bias', 'roberta.roberta.encoder.layer.5.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.value.weight', 'roberta.roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.intermediate.dense.weight', 'roberta_m.roberta.embeddings.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.output.dense.weight', 'roberta.roberta.encoder.layer.3.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.key.weight', 'roberta.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.attention.self.key.weight', 'roberta.roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.roberta.encoder.layer.6.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.4.attention.self.key.bias', 'roberta.roberta.encoder.layer.2.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.0.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.attention.self.query.bias', 'roberta.roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.roberta.encoder.layer.3.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.6.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.value.weight', 'roberta.roberta.encoder.layer.0.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.attention.self.value.bias', 'roberta.roberta.encoder.layer.1.attention.self.key.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.key.weight', 'roberta.roberta.encoder.layer.2.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.roberta.pooler.dense.bias', 'roberta_m.roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.roberta.encoder.layer.11.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.0.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.key.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.attention.self.query.weight', 'roberta.roberta.embeddings.token_type_embeddings.weight', 'roberta.roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.8.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.9.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.value.bias', 'roberta.roberta.pooler.dense.weight', 'roberta.roberta.encoder.layer.2.attention.self.query.weight', 'roberta.roberta.encoder.layer.6.output.dense.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.8.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.roberta.encoder.layer.10.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.11.output.dense.weight', 'roberta.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.query.weight', 'roberta.roberta.encoder.layer.7.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.2.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.6.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.3.attention.self.value.weight', 'roberta.roberta.encoder.layer.5.attention.self.value.bias', 'roberta.roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.roberta.encoder.layer.11.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta_m.roberta.embeddings.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.roberta.encoder.layer.10.output.dense.bias', 'roberta.roberta.encoder.layer.2.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.10.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.value.bias', 'roberta.roberta.encoder.layer.10.attention.self.key.bias', 'roberta.roberta.encoder.layer.10.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.2.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.key.weight', 'roberta.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.output.dense.bias', 'roberta.roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.5.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.5.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.5.output.dense.bias', 'roberta.roberta.encoder.layer.6.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.key.bias', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.8.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.0.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.1.output.dense.bias', 'roberta.roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.value.bias', 'roberta.roberta.encoder.layer.6.attention.self.query.weight', 'roberta.roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.roberta.encoder.layer.4.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.query.weight', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.6.output.dense.weight', 'roberta.roberta.encoder.layer.7.attention.self.value.bias', 'roberta.roberta.encoder.layer.0.attention.self.key.weight', 'roberta.roberta.encoder.layer.0.attention.self.query.bias', 'roberta.roberta.encoder.layer.6.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.dense.bias', 'roberta_m.roberta.pooler.dense.weight', 'roberta.roberta.encoder.layer.11.attention.self.key.bias', 'roberta.roberta.encoder.layer.2.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.key.weight', 'roberta.roberta.encoder.layer.5.output.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.value.bias', 'roberta.roberta.encoder.layer.1.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.4.output.dense.weight', 'roberta.roberta.encoder.layer.8.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.8.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.key.bias', 'roberta.roberta.embeddings.position_ids', 'roberta_m.roberta.encoder.layer.11.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.query.weight', 'roberta.roberta.encoder.layer.1.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.value.bias', 'roberta.roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.attention.self.query.bias', 'roberta.roberta.encoder.layer.9.output.dense.bias', 'roberta_m.roberta.encoder.layer.0.output.dense.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.value.weight', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.4.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.roberta.embeddings.word_embeddings.weight', 'roberta_m.roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.3.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.value.weight', 'roberta.roberta.embeddings.LayerNorm.bias', 'roberta.roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.11.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.roberta.encoder.layer.7.output.dense.bias', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.key.bias', 'roberta.roberta.encoder.layer.0.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.attention.self.value.bias', 'roberta.roberta.encoder.layer.4.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.roberta.encoder.layer.6.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.6.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.embeddings.word_embeddings.weight', 'roberta_m.roberta.encoder.layer.2.attention.self.key.bias', 'roberta.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.10.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.1.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.4.output.dense.bias', 'roberta.roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.key.weight', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.query.bias', 'roberta.roberta.encoder.layer.3.attention.self.key.bias', 'roberta.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.3.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.2.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.3.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.1.output.dense.weight', 'roberta_m.roberta.embeddings.position_ids', 'roberta_m.roberta.encoder.layer.10.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.query.bias', 'roberta.roberta.encoder.layer.1.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.key.bias', 'roberta.roberta.embeddings.position_embeddings.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.4.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.roberta.encoder.layer.7.attention.self.value.weight', 'roberta.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.1.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.6.output.dense.bias', 'roberta.roberta.encoder.layer.6.attention.self.value.bias', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.0.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.1.intermediate.dense.weight', 'roberta_m.roberta.pooler.dense.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.query.bias', 'roberta.roberta.encoder.layer.8.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.value.bias', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.5.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.10.attention.self.value.weight', 'roberta.roberta.encoder.layer.5.attention.self.query.weight', 'roberta.roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.0.output.dense.bias', 'roberta.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.roberta.encoder.layer.4.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.query.weight', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.11.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.query.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./trained_model_wiki and are newly initialized: ['roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,RobertaTokenizer # Load the trained model \n",
    "\n",
    "wiki_model = AutoModel.from_pretrained('./trained_model_wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_allsides_dataset import allsidesData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import argparse\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from load_wiki_dataset import wikiData\n",
    "from losses import align_loss, uniform_loss\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "def train_allsides(args, model_to_train):\n",
    "    # Load dataset\n",
    "    with open(args.data_path, 'r', encoding='UTF-8') as f:\n",
    "        input_text = f.readlines()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # ADD THIS LINE\n",
    "\n",
    "    allsides = allsidesData(input_text, tokenizer)  # PASS tokenizer TO wikiData\n",
    "    train_params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    trainloader = DataLoader(allsides, **train_params)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = model_to_train\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    for epoch in range(args.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}  # move batch to device\n",
    "                # print(model(batch))\n",
    "                loss, cls = model(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                # print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/allsides.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 65720/65720 [3:01:13<00:00,  6.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.609909778521399\n"
     ]
    }
   ],
   "source": [
    "allsides_wiki_model = train_allsides(args,wiki_model2)\n",
    "    # Save the trained model if needed\n",
    "allsides_wiki_model.save_pretrained('./allsides_wiki_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(allsides_wiki_model.state_dict(), 'allsides_wiki_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  70%|██████▉   | 86938/125000 [3:55:10<1:42:57,  6.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m allsides_wiki2_model \u001b[39m=\u001b[39m train_wiki(args,allsides_wiki_model)\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Save the trained model if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m allsides_wiki2_model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m./allsides_wiki2_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 61\u001b[0m, in \u001b[0;36mtrain_wiki\u001b[1;34m(args, model)\u001b[0m\n\u001b[0;32m     59\u001b[0m             loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 61\u001b[0m             epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     62\u001b[0m             \u001b[39m# print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(trainloader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "allsides_wiki2_model = train_wiki(args,allsides_wiki_model)\n",
    "    # Save the trained model if needed\n",
    "allsides_wiki2_model.save_pretrained('./allsides_wiki2_model')\n",
    "torch.save(allsides_wiki2_model.state_dict(), 'allsides_wiki2_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "wiki_model = CustomRobertaModel()\n",
    "wiki_model.load_state_dict(torch.load('1wiki.pth'))\n",
    "allsides_wiki_model = CustomRobertaModel()\n",
    "allsides_wiki_model.load_state_dict(torch.load('allsides_wiki_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "simcse_1wiki_model = CustomRobertaModel()\n",
    "simcse_1wiki_model.load_state_dict(torch.load('simcse_1wiki_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125000/125000 [5:35:28<00:00,  6.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.8066049778327944\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'allsides_wiki2_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Save the trained model if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m simcse_1wiki_model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m./simcse_2wiki_model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39msave(allsides_wiki2_model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39msimcse_2wiki_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'allsides_wiki2_model' is not defined"
     ]
    }
   ],
   "source": [
    "simcse_2wiki_model = train_wiki(args,simcse_1wiki_model)\n",
    "    # Save the trained model if needed\n",
    "simcse_2wiki_model.save_pretrained('./simcse_2wiki_model')\n",
    "torch.save(simcse_2wiki_model.state_dict(), 'simcse_2wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_2wiki_model.save_pretrained('./simcse_2wiki_model')\n",
    "torch.save(simcse_2wiki_model.state_dict(), 'simcse_2wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomRobertaModel:\n\tUnexpected key(s) in state_dict: \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRoBERTa\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomRobertaModel\n\u001b[0;32m      2\u001b[0m simcse_2wiki_model \u001b[39m=\u001b[39m CustomRobertaModel()\n\u001b[1;32m----> 3\u001b[0m simcse_2wiki_model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39msimcse_2wiki_model.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomRobertaModel:\n\tUnexpected key(s) in state_dict: \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\". "
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "simcse_2wiki_model = CustomRobertaModel()\n",
    "simcse_2wiki_model.load_state_dict(torch.load('simcse_2wiki_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125000/125000 [5:38:20<00:00,  6.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.821558924482346\n"
     ]
    }
   ],
   "source": [
    "simcse_3wiki_model = train_wiki(args,simcse_2wiki_model)\n",
    "    # Save the trained model if needed\n",
    "simcse_3wiki_model.save_pretrained('./simcse_3wiki_model')\n",
    "torch.save(simcse_3wiki_model.state_dict(), 'simcse_3wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "real_simcse_wiki_model = CustomRobertaModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 2\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 125000/125000 [8:50:44<00:00,  3.93it/s]   \n",
      "Epoch 2/2: 100%|██████████| 125000/125000 [8:40:10<00:00,  4.01it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5896223703017234\n"
     ]
    }
   ],
   "source": [
    "real_simcse_2wiki_model  = train_wiki(args, real_simcse_wiki_model)\n",
    "torch.save(real_simcse_2wiki_model.state_dict(), 'real_simcse_2wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1: 100%|██████████| 125000/125000 [8:13:29<00:00,  4.22it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.6381619698782004\n"
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "real_simcse_wiki_model = CustomRobertaModel()\n",
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\"\n",
    "real_simcse_1wiki_model  = train_wiki(args, real_simcse_wiki_model)\n",
    "torch.save(real_simcse_1wiki_model.state_dict(), 'real_simcse_1wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_moreal_simcse_wiki_model = CustomRobertaModel()\n",
    "try_moreal_simcse_wiki_model.load_state_dict(torch.load('real_simcse_2wiki_model.pth'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdf77093bbdbf1309a6eb466c68765f3f88bd63ff87e3bfe863ca62958fcd8bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
