{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing CustomRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CustomRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b607b549fb444cfb3e127c08687964e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install adapter-transformers\n",
    "from transformers import RobertaModel, list_adapters\n",
    "from RoBERTa import *\n",
    "import torch\n",
    "\n",
    "custom_roberta = CustomRobertaModel.from_pretrained('roberta-base')\n",
    "adapter_name = custom_roberta.load_adapter(\"AdapterHub/bert-base-uncased-pf-imdb\", source=\"hf\")\n",
    "\n",
    "custom_roberta.active_adapters = adapter_name\n",
    "for name, param in custom_roberta.named_parameters():\n",
    "    if f\"adapters.{adapter_name}\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RobertaModel.forward() got an unexpected keyword argument 'adapter_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n\u001b[1;32m---> 12\u001b[0m outputs1 \u001b[39m=\u001b[39m custom_roberta(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Get the CLS output (the first token embedding)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m cls_output \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Ben\\Documents\\EECS487\\proj\\model\\RoBERTa.py:12\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, adapter_names, adapter_args, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m from transformers import RobertaModel, RobertaConfig\n\u001b[0;32m      3\u001b[0m from transformers import RobertaConfig, RobertaModelWithHeads\n\u001b[0;32m      5\u001b[0m # class CustomRobertaModel(RobertaModel):\n\u001b[0;32m      6\u001b[0m #     def __init__(self, config: RobertaConfig, dropout_prob: float = 0.1):\n\u001b[0;32m      7\u001b[0m #         super().__init__(config)\n\u001b[0;32m      8\u001b[0m #         self.dropout = torch.nn.Dropout(dropout_prob)\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m #     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None, adapter_names=None, adapter_args=None, **kwargs):\n\u001b[0;32m     11\u001b[0m #         # Call the original forward method of the RobertaModelWithHeads\n\u001b[1;32m---> 12\u001b[0m #         outputs = super().forward(\n\u001b[0;32m     13\u001b[0m #             input_ids=input_ids,\n\u001b[0;32m     14\u001b[0m #             attention_mask=attention_mask,\n\u001b[0;32m     15\u001b[0m #             token_type_ids=token_type_ids,\n\u001b[0;32m     16\u001b[0m #             position_ids=position_ids,\n\u001b[0;32m     17\u001b[0m #             head_mask=head_mask,\n\u001b[0;32m     18\u001b[0m #             inputs_embeds=inputs_embeds,\n\u001b[0;32m     19\u001b[0m #             output_attentions=output_attentions,\n\u001b[0;32m     20\u001b[0m #             output_hidden_states=output_hidden_states,\n\u001b[0;32m     21\u001b[0m #             return_dict=return_dict,\n\u001b[0;32m     22\u001b[0m #             adapter_names=adapter_names,\n\u001b[0;32m     23\u001b[0m #             adapter_args=adapter_args,\n\u001b[0;32m     24\u001b[0m #             **kwargs\n\u001b[0;32m     25\u001b[0m #         )\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m #         # Apply dropout to the last hidden state\n\u001b[0;32m     28\u001b[0m #         last_hidden_state = self.dropout(outputs.last_hidden_state)\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m #         # Replace the last_hidden_state in the outputs\n\u001b[0;32m     31\u001b[0m #         outputs = outputs.update(last_hidden_state=last_hidden_state)\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m #         return outputs\n\u001b[0;32m     34\u001b[0m class CustomRobertaModel(RobertaModel):\n\u001b[0;32m     35\u001b[0m     def __init__(self, config: RobertaConfig, dropout_rate=0.1):\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\context.py:108\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m ctx:\n\u001b[0;32m    105\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    106\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39moutput_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcontext_attributes\n\u001b[0;32m    107\u001b[0m     }\n\u001b[1;32m--> 108\u001b[0m     results \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    110\u001b[0m     \u001b[39m# append output attributes\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: RobertaModel.forward() got an unexpected keyword argument 'adapter_names'"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "input_text = \"This is an example sentence.\"\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoded_input)\n",
    "outputs1 = custom_roberta(**encoded_input)\n",
    "\n",
    "# Get the CLS output (the first token embedding)\n",
    "cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "cls_output1 = outputs1.last_hidden_state[:, 0, :]\n",
    "print(cls_output[0,0])\n",
    "print(cls_output1[0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
