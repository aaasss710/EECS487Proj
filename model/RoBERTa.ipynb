{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d27eb5298442789dbceab96b4d87df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from RoBERTa import *\n",
    "model = CustomRobertaModel(adapter_name=\"AdapterHub/bert-base-uncased-pf-imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRobertaModel(\n",
       "  (shared_parameters): ModuleDict()\n",
       "  (invertible_adapters): ModuleDict()\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (prefix_tuning): PrefixTuningPool(\n",
       "    (prefix_tunings): ModuleDict()\n",
       "  )\n",
       "  (roberta): RobertaModel(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.9294e-02,  3.5373e-02, -1.9225e-02, -1.4038e-01,  4.7057e-02,\n",
      "         -7.0479e-02, -5.2851e-02,  7.8655e-03,  7.3856e-02, -7.4399e-02,\n",
      "         -2.1161e-02,  5.5085e-02,  2.5212e-02, -5.6436e-03,  7.0579e-02,\n",
      "          5.0041e-02, -8.1089e-02,  2.0510e-02, -3.1904e-03, -2.8745e-02,\n",
      "         -1.0839e-01,  4.4738e-02, -3.6618e-02,  1.1364e-01,  9.9266e-03,\n",
      "          7.2116e-03,  4.7177e-02,  1.6550e-02, -6.9273e-02, -1.0080e-03,\n",
      "         -5.1527e-02,  9.3726e-03,  2.2701e-02, -1.5651e-02,  5.7705e-03,\n",
      "          1.3527e-01,  4.0986e-02,  3.6100e-02, -1.2358e-01,  1.8546e-02,\n",
      "         -2.7680e-03,  7.0516e-02, -1.6954e-03,  1.2969e-02,  7.3284e-02,\n",
      "          4.3084e-02,  1.3169e-03, -5.4099e-03, -6.3367e-02,  4.7515e-02,\n",
      "          2.2799e-02,  8.1599e-02, -2.0455e-02,  1.7465e-03, -8.1086e-02,\n",
      "          2.4256e-02,  2.2455e-02,  1.0539e-01,  1.0426e-01, -4.2001e-02,\n",
      "          1.9064e-02, -9.7238e-02, -1.0575e-01,  1.9755e-02, -2.9074e-02,\n",
      "         -1.2981e-02, -5.8216e-02,  4.7608e-02,  1.7968e-02,  2.4385e-02,\n",
      "          7.1549e-02, -4.3143e-02,  4.5332e-02, -2.7885e-02,  6.7027e-03,\n",
      "          1.7348e-02,  3.0419e-02,  5.6879e-01, -5.8020e-02,  1.5023e-02,\n",
      "         -1.3461e-03, -1.9422e-02,  3.1865e-01,  5.6381e-02, -6.8798e-03,\n",
      "         -2.7110e-02,  9.5726e-02,  2.4757e-02,  1.0014e-02, -5.4212e-04,\n",
      "          3.7445e-02,  4.9686e-02, -4.6488e-02,  2.3746e-02,  2.5302e-02,\n",
      "          4.4201e-02,  1.9931e-03,  1.3417e-02, -3.6464e-02, -3.3635e-02,\n",
      "         -4.1704e-02, -6.4198e-02,  1.0130e-01,  3.2168e-02,  4.3065e-02,\n",
      "          6.1425e-02,  5.5696e-03,  6.6017e-02, -1.9959e-02, -3.5303e-02,\n",
      "         -2.5259e-02,  3.3221e-02, -1.0627e-02,  2.9385e-02, -4.4346e-03,\n",
      "         -5.6831e-02, -3.3157e-02,  2.8513e-02,  1.5687e-02,  6.9865e-02,\n",
      "         -1.6961e-02,  2.3675e-02,  1.2817e-01, -2.3585e-02, -3.6503e-02,\n",
      "         -4.2534e-02, -4.7631e-02, -1.1075e-03, -1.1106e-02,  4.6812e-02,\n",
      "          8.7043e-03, -1.0897e-01, -4.6112e-04,  7.1508e-02,  7.5716e-02,\n",
      "          3.7496e-02,  6.9279e-02, -2.1490e-02,  8.2746e-02,  2.0936e-02,\n",
      "         -3.8268e-02,  7.9075e-02,  6.3451e-02,  5.4271e-02,  1.0313e-01,\n",
      "         -2.5534e-02, -3.3211e-03, -3.9894e-02,  3.9035e-02, -2.5070e-02,\n",
      "          9.1484e-02, -6.0711e-02, -3.5896e-02,  6.6127e-02, -3.6169e-02,\n",
      "          3.5451e-01,  4.6160e-02,  1.1008e-02, -3.6886e-02,  7.1630e-02,\n",
      "          1.4517e-01,  6.6376e-03,  2.0445e-02, -9.5518e-03, -2.7836e-02,\n",
      "          2.1101e-03,  1.0939e-03,  2.1336e-02,  5.7719e-02,  3.2134e-03,\n",
      "          3.6355e-02,  3.2873e-02,  2.6282e-02, -7.1189e-02, -5.3365e-02,\n",
      "         -9.0151e-02, -3.6971e-02, -1.9480e-02, -6.2387e-02,  5.2321e-03,\n",
      "          2.0071e-02,  6.0990e-02, -5.1205e-02,  2.6477e-02, -1.4892e-02,\n",
      "          3.1052e-02, -2.2441e-02,  1.7989e-02, -1.8604e-02, -3.7246e-02,\n",
      "          2.2649e-02, -4.9544e-02,  1.6908e-02,  1.1387e-02,  1.0536e-02,\n",
      "          8.9312e-02, -1.7611e-02, -3.3191e-02,  1.5458e-02, -1.6512e-02,\n",
      "          2.7756e-02, -8.6067e-02,  8.4190e-02, -3.5071e-02,  7.5453e-02,\n",
      "         -1.2669e-02,  2.0806e-02,  2.3340e-02,  4.8516e-02, -9.0396e-02,\n",
      "         -5.6254e-02,  7.9841e-02, -3.2031e-02,  7.5929e-02,  2.7480e-02,\n",
      "         -1.2178e-02,  6.0948e-02,  1.0727e-01,  5.3739e-03, -4.8043e-02,\n",
      "          2.3723e-02,  6.3128e-02,  8.2434e-03,  9.1537e-02, -2.5735e-02,\n",
      "          1.2479e-02,  1.7525e-02, -6.0858e-04,  1.6253e-02, -2.9860e-02,\n",
      "          9.2782e-03,  3.7045e-02,  2.5051e-02, -2.2824e-02,  5.1982e-02,\n",
      "         -1.0922e-01, -1.2087e-02, -3.3996e-02, -5.7234e-02,  5.0698e-02,\n",
      "         -9.3108e-02,  6.0491e-02,  6.3303e-03, -3.4166e-03,  1.5517e-02,\n",
      "          4.2454e-02,  2.0623e-03,  1.4776e-01,  6.7250e-03,  7.5648e-02,\n",
      "         -1.3794e-02,  1.3540e-02, -2.2509e-02, -3.1601e-02,  2.6682e-02,\n",
      "         -1.2349e-02, -7.7991e-02,  4.1985e-03, -6.4949e-03,  1.9191e-02,\n",
      "          1.6506e-03, -2.2915e-02,  3.5450e-02, -2.4512e-02, -1.2773e-01,\n",
      "         -1.2699e-02, -4.7247e-03,  5.3909e-02,  9.3635e-03, -4.1564e-02,\n",
      "         -1.5822e-02, -4.8698e-02, -4.2099e-02, -4.0644e-03,  1.9738e-04,\n",
      "          6.6508e-02, -1.2236e-01,  1.3965e-02, -4.1108e-02, -1.8787e-02,\n",
      "          3.8810e-02, -2.9111e-02, -9.3350e-02, -2.0290e-02, -2.3343e-04,\n",
      "          3.1319e-02, -5.1661e-02, -6.1283e-03,  3.0473e-02,  6.7780e-02,\n",
      "          7.0441e-02,  6.2052e-02, -2.9088e-02,  3.1189e-02, -2.7579e-02,\n",
      "          5.0268e-02,  7.6755e-02,  1.0989e-02,  4.6928e-03,  1.1827e-02,\n",
      "         -8.8408e-03, -1.0896e-01,  2.4561e-02,  1.7095e-02,  1.4683e-02,\n",
      "         -5.3782e-02,  9.1613e-03, -3.6498e-02,  5.3141e-02, -1.5383e-02,\n",
      "         -4.1883e-02, -2.2138e-02, -1.0111e-01,  1.2388e-01,  9.4792e-02,\n",
      "         -5.4291e-03,  3.3129e-02, -2.3241e-02,  4.9126e-03,  6.3498e-02,\n",
      "          1.7792e-02,  1.7654e-02, -1.1147e-02,  2.7881e-03, -3.2339e-02,\n",
      "          6.4006e-02,  6.3030e-02, -7.3351e-03,  4.6392e-02,  4.2839e-01,\n",
      "         -3.2342e-01,  2.6834e-02,  7.9420e-02,  1.2489e-02,  9.9511e-03,\n",
      "          6.5029e-02,  5.0001e-02,  8.7458e-03,  7.3447e-02,  5.8215e-02,\n",
      "          1.4571e-02,  8.5409e-02, -3.6386e-02,  2.8650e-02,  3.2667e-02,\n",
      "          9.4371e-04,  1.7473e-02,  1.5824e-03, -1.0474e-02,  2.0175e-02,\n",
      "         -2.7489e-02, -4.8832e-02,  4.6175e-02, -2.5152e-02,  3.6160e-02,\n",
      "          7.1518e-02,  4.4099e-02, -2.5001e-02, -6.2887e-03,  6.3250e-03,\n",
      "          2.2533e-02,  1.4252e-02,  2.7530e-02, -2.5810e-02,  1.2562e-01,\n",
      "         -7.9569e-02, -1.3238e-02,  5.2861e-02,  2.4813e-02,  1.2697e-03,\n",
      "          8.0116e-02, -7.2457e-02, -6.6419e-02,  4.7214e-02,  2.3361e-02,\n",
      "          6.6377e-03,  2.0747e-02,  2.6059e-03, -2.8432e-03,  1.0724e-01,\n",
      "          3.4883e-02,  9.7589e-03, -5.7026e-02, -6.9154e-04,  4.5560e-02,\n",
      "         -8.4153e-02,  1.4819e-02,  9.2166e-03,  7.3389e-02,  7.6180e-04,\n",
      "         -9.4480e-03, -8.5372e-02, -2.2464e-02, -9.5923e-03,  8.9640e-02,\n",
      "         -2.5809e-02, -2.9349e-02, -1.2395e-01, -5.7249e-02, -8.8032e-02,\n",
      "          1.6195e-02,  3.4811e-02, -1.1555e-01, -5.2370e-03, -1.2255e-02,\n",
      "          2.7499e-02,  1.1863e-02, -5.9111e-02,  2.6009e-02, -7.7281e-02,\n",
      "         -3.1452e-02,  5.4236e-02,  4.4002e-02, -4.7998e-02, -1.8927e-03,\n",
      "         -2.3529e-02,  3.8726e-02, -3.1702e-02, -4.7609e-02,  2.5345e-02,\n",
      "         -5.1632e-02,  5.9701e-02,  2.0341e-02,  2.6221e-02, -2.9651e-02,\n",
      "          1.2007e-02,  2.6901e-02, -5.4761e-02,  4.3534e-03,  5.4776e-03,\n",
      "         -9.2653e-03,  5.0503e-02, -2.1618e-02, -9.7861e-03, -1.5472e-02,\n",
      "         -3.6807e-02,  4.7965e-02, -2.3690e-02, -1.8124e-02, -4.9751e-03,\n",
      "         -1.0932e-02, -3.0206e-02,  2.2704e-02,  7.7053e-02, -5.2511e-02,\n",
      "          3.8165e-03,  1.3543e-01,  7.0549e-03, -4.7965e-02,  6.6768e-03,\n",
      "         -3.0009e-02,  2.2850e-02, -2.5906e-02, -5.0848e-01,  6.6225e-02,\n",
      "          2.6747e-02, -2.6207e-02,  3.3411e-02, -8.3839e-02, -1.2679e-02,\n",
      "          1.6241e-02,  1.8062e-02,  8.1428e-02, -5.5005e-02,  2.8984e-02,\n",
      "         -3.3852e-02, -5.8660e-02,  1.9425e-02, -2.4689e-02, -2.4959e-02,\n",
      "         -2.1161e-03, -2.1890e-02, -1.0339e-02, -6.6597e-02,  1.9528e-02,\n",
      "          2.4449e-02, -2.6408e-02,  7.6739e-02, -2.8672e-02, -6.0520e-02,\n",
      "         -2.7392e-02,  4.3716e-02,  7.6328e-02, -1.5674e-02, -7.3715e-02,\n",
      "         -2.1457e-02,  1.8384e-02,  1.0642e-02,  8.3116e-02,  1.0138e-02,\n",
      "         -3.7734e-02, -5.1477e-02,  2.2848e-02,  4.1191e-02,  2.3068e-01,\n",
      "          1.5463e-02,  1.9991e-01,  4.8445e-02,  6.5159e-02, -1.0963e-02,\n",
      "          9.5439e-03,  2.7772e-02,  1.0342e-02,  7.6126e-03, -4.2058e-02,\n",
      "         -1.0220e-03, -6.4575e-02, -4.4468e-02,  2.2870e-02, -6.7813e-03,\n",
      "         -5.9791e-02,  2.5887e-02,  1.3299e-01, -1.0075e-02,  2.3873e-02,\n",
      "          1.7304e-02,  4.6384e-03, -4.4041e-02, -8.3175e-03, -1.7985e-03,\n",
      "         -5.1508e-02, -3.4280e-03, -2.8190e-02, -7.7340e-02, -6.9681e-02,\n",
      "         -2.0903e-02,  2.1940e-02,  2.8811e-02,  9.1827e-02,  2.2985e-03,\n",
      "          3.8012e-02, -4.8029e-02,  6.1236e-02,  2.3032e-02,  3.4183e-02,\n",
      "          4.7473e-02,  1.1236e-01,  3.3814e-02, -5.6270e-02, -5.8646e-02,\n",
      "         -1.8851e-02,  1.7822e-02,  9.5016e-02, -3.1109e-02,  1.3790e-01,\n",
      "          4.0624e-02, -6.7961e-03,  2.4916e-03,  4.2061e-02,  3.0674e-02,\n",
      "         -3.1422e-02, -6.8201e-01, -4.6999e-02,  6.6970e-02,  3.2648e-02,\n",
      "          9.5271e-03,  6.2267e-02,  2.4761e-02, -4.8586e-02,  7.2237e-02,\n",
      "         -1.6670e-02,  7.0758e-03,  5.3857e-02,  1.4315e-01, -3.7874e-02,\n",
      "          1.9475e-02,  3.9727e-02, -4.5405e-02, -6.1603e-02,  2.1877e-02,\n",
      "         -2.7750e-01, -2.1007e-03, -6.7154e-02,  1.0905e-01, -6.4081e-03,\n",
      "          5.3738e-02,  6.4291e-02,  1.6585e-02,  9.3298e-02,  6.3210e-02,\n",
      "          7.5805e-02,  6.5783e-02,  1.5910e-02, -3.8443e-02, -5.9108e-03,\n",
      "         -5.7331e-02,  7.8008e-02,  7.4938e-02,  1.0164e+01, -2.6780e-02,\n",
      "          5.5975e-02, -6.6694e-03, -3.6417e-02, -6.9883e-02,  5.3365e-02,\n",
      "         -5.2129e-02, -2.8689e-02,  7.9191e-02,  2.9780e-02, -4.6193e-04,\n",
      "         -7.1788e-02,  9.9619e-03,  3.0480e-02,  3.1891e-02, -6.5682e-02,\n",
      "         -5.5614e-02,  5.9176e-02,  8.3449e-03, -1.3829e-02,  1.9291e-02,\n",
      "          5.3136e-03, -5.6498e-02, -6.4918e-02,  5.0155e-02,  5.3141e-02,\n",
      "         -3.4562e-02, -2.6675e-02, -8.9800e-03,  1.9949e-03,  4.8572e-02,\n",
      "          3.2176e-02,  9.7721e-03,  7.6742e-02, -6.7973e-03,  4.4028e-02,\n",
      "          5.0185e-02, -3.1333e-03,  7.6988e-02,  2.3896e-02,  2.1833e-02,\n",
      "          3.4534e-02, -4.9020e-02,  8.3504e-02,  4.0195e-02, -3.5123e-02,\n",
      "          7.2345e-02,  7.4889e-04, -7.5826e-04,  5.9881e-02, -6.9593e-02,\n",
      "          6.9803e-02,  1.2962e-02, -2.0977e-02,  1.3645e-02,  5.5834e-02,\n",
      "         -2.5125e-02,  3.9858e-02,  4.7505e-02, -6.2372e-02,  8.4086e-02,\n",
      "         -2.5750e-02,  2.4982e-02, -6.5203e-02,  8.2113e-02,  6.5378e-02,\n",
      "          1.2142e-01, -6.6097e-02,  1.7292e-02, -3.8678e-02, -8.8843e-02,\n",
      "         -4.6773e-02, -4.1791e-02,  3.4017e-02, -9.6279e-02,  3.1062e-02,\n",
      "          6.1974e-03,  1.8532e-02, -5.8147e-02,  3.2549e-02,  9.0308e-03,\n",
      "         -1.7951e-02, -8.1236e-03,  7.7675e-02, -7.3618e-03,  5.2293e-02,\n",
      "          1.0596e-01, -2.4297e-02, -1.5469e-02, -5.5737e-02,  4.2822e-02,\n",
      "         -2.7263e-02, -3.8394e-02,  2.5111e-02,  2.0704e-02,  3.5095e-03,\n",
      "         -7.3659e-02,  3.4964e-02,  4.3484e-02, -6.7348e-02,  5.1461e-02,\n",
      "         -1.0327e-02,  7.2659e-02, -5.2165e-03,  4.2701e-02, -1.3215e-02,\n",
      "         -7.9928e-03,  4.3516e-02, -1.5292e-02, -2.3564e-02,  1.2837e-02,\n",
      "         -4.7845e-02, -2.5832e-02,  3.1179e-02,  3.6606e-02,  1.3375e-02,\n",
      "          6.4986e-02,  1.2902e-02,  7.1481e-02, -6.0919e-02,  1.4252e-02,\n",
      "         -2.1960e-02, -3.6240e-02, -4.7407e-02, -5.5091e-02, -2.6801e-02,\n",
      "          1.0669e-03,  5.1332e-02,  1.9226e-02, -4.3680e-02,  2.2196e-02,\n",
      "          3.4863e-02,  1.4846e-03,  2.5279e-02, -2.0511e-02, -2.3173e-02,\n",
      "          6.1961e-02,  6.5151e-02,  5.8395e-02, -2.5875e-02, -2.1122e-02,\n",
      "         -2.3783e-02, -1.1983e-01, -1.3343e-02,  3.3462e-02,  7.8789e-02,\n",
      "          3.8605e-02,  4.5091e-02, -2.9910e-02,  1.4676e-02,  2.0137e-02,\n",
      "         -2.1506e-02, -3.5160e-03, -1.6790e-03, -2.7255e-02,  5.1344e-03,\n",
      "          5.4678e-02, -5.3577e-03, -3.4894e-02,  3.8988e-02, -2.6407e-02,\n",
      "          2.3976e-02,  1.9956e-02, -3.6659e-02, -5.1951e-02, -1.3572e-02,\n",
      "          7.9640e-03,  3.1242e-03, -1.5776e-02,  1.1683e-02,  1.6783e-03,\n",
      "         -6.9599e-02, -8.0731e-02, -5.2039e-03,  1.1505e-01,  7.3365e-02,\n",
      "         -5.8426e-02, -7.3970e-02, -4.4468e-03]])\n"
     ]
    }
   ],
   "source": [
    "text = \"This movie was amazing!\"\n",
    "cls = model(text)\n",
    "print(cls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
