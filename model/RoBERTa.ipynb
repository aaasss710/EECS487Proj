{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from RoBERTa import *\n",
    "# model = CustomRobertaModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# input_tokens =tokenizer(\"test and test.\", return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "# input_tokens = input_tokens.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0255e-01,  1.0369e-01, -1.0960e-02, -5.4950e-02,  1.2176e-01,\n",
      "         -3.2302e-03, -2.3434e-02,  4.3359e-02,  5.0298e-02, -8.6572e-02,\n",
      "         -2.3857e-02,  3.4339e-02,  4.3145e-02, -7.7734e-02,  5.4695e-02,\n",
      "         -1.0053e-01, -4.0279e-02, -5.7762e-03, -1.4725e-02, -4.9593e-02,\n",
      "         -1.3021e-01,  2.5552e-02,  2.0522e-02,  1.0200e-01, -6.3678e-02,\n",
      "          8.8177e-02,  1.4348e-01,  5.3061e-02, -7.1654e-02,  5.3694e-02,\n",
      "         -6.8900e-02, -9.9970e-02,  8.2682e-02, -1.3511e-02,  2.0117e-02,\n",
      "          1.3400e-01,  2.7797e-03, -8.3667e-04, -5.8601e-02,  5.1972e-02,\n",
      "          1.2828e-02,  2.1757e-01,  1.7009e-02, -4.6349e-02,  6.8069e-02,\n",
      "          3.1592e-02,  3.6040e-02, -2.5366e-02,  1.2174e-02, -1.6848e-02,\n",
      "          4.5981e-03,  5.4572e-02, -2.1784e-02,  5.7348e-02, -1.3289e-01,\n",
      "          6.8254e-02,  3.0545e-02,  7.2083e-02,  8.2089e-02, -1.2538e-01,\n",
      "         -3.0586e-02, -1.8470e-01, -6.9079e-02, -6.0674e-02,  5.5125e-02,\n",
      "         -1.0963e-01, -3.8253e-02,  5.7974e-02,  2.5209e-02,  5.6676e-02,\n",
      "          6.6650e-02, -5.7630e-02, -7.0528e-03,  1.0601e-02,  3.2166e-02,\n",
      "         -4.5240e-03, -1.8739e-02,  5.8682e-01, -2.7962e-02,  1.2938e-02,\n",
      "          6.2185e-02, -8.3002e-02,  5.8404e-01,  5.5627e-02, -4.2669e-03,\n",
      "         -5.6232e-02,  1.4140e-01, -5.6821e-03,  4.3118e-02,  7.9804e-02,\n",
      "         -4.8317e-02,  7.8521e-02, -4.1143e-02,  1.0308e-02,  5.1249e-02,\n",
      "          3.2183e-03, -2.3498e-02,  1.2585e-01, -9.8542e-02, -7.0455e-02,\n",
      "         -2.6493e-02, -6.5842e-02,  1.0553e-01,  8.8572e-02, -2.0663e-02,\n",
      "         -7.9825e-03,  9.0078e-02, -3.3579e-02,  4.0537e-02, -4.6728e-02,\n",
      "         -2.1361e-02, -2.6896e-03,  6.8196e-02, -2.1516e-02,  3.5645e-02,\n",
      "         -2.5768e-02,  1.9392e-02,  8.0571e-02,  5.7537e-02,  3.3482e-02,\n",
      "         -3.4843e-02,  6.3484e-02,  9.8376e-02, -6.1567e-02, -7.0961e-02,\n",
      "         -1.0034e-01, -5.0497e-02, -9.9992e-03,  7.0938e-03,  1.6755e-02,\n",
      "         -4.6749e-02, -1.7698e-01,  9.0778e-04,  8.4737e-02,  1.5545e-02,\n",
      "          1.1887e-02,  2.6032e-02, -4.9570e-02,  2.0886e-02, -9.2508e-02,\n",
      "         -1.9673e-03,  3.5068e-02,  5.0853e-02,  1.3869e-02,  1.1944e-01,\n",
      "          8.0163e-02, -5.3810e-02, -2.0149e-02,  4.8207e-02,  5.3597e-03,\n",
      "          1.2693e-01, -5.9935e-02, -3.7519e-02, -3.4062e-02, -5.1985e-02,\n",
      "          5.3274e-01,  1.1878e-01,  1.5434e-01,  1.7108e-02,  1.4908e-02,\n",
      "          2.1055e-01,  6.0076e-02,  3.1167e-02, -3.8785e-02, -9.2120e-03,\n",
      "         -1.6625e-02, -6.8709e-02, -7.0710e-02,  3.8311e-02,  1.9385e-02,\n",
      "          6.8637e-02,  3.7470e-02,  7.3401e-03, -1.3814e-02, -1.1375e-01,\n",
      "         -6.1690e-02,  6.5720e-02,  4.1201e-03, -1.1441e-01,  3.9362e-02,\n",
      "          8.4953e-03,  1.0701e-01, -1.1664e-01,  5.6977e-03, -2.3933e-02,\n",
      "          6.8238e-02,  2.4265e-02, -1.0737e-02, -3.4237e-02,  7.3842e-02,\n",
      "          1.0543e-01, -3.5569e-02, -3.6212e-02, -2.5952e-02, -2.9731e-02,\n",
      "          8.8929e-02, -8.2062e-03, -3.3984e-03,  2.7404e-02, -5.5331e-02,\n",
      "          8.1087e-03, -6.8448e-02,  1.1682e-01, -1.1967e-01,  2.9929e-02,\n",
      "         -2.3187e-02,  3.2698e-03,  7.8010e-02,  7.1490e-04, -9.3949e-02,\n",
      "          9.1565e-03,  9.6677e-03, -3.8123e-02,  7.9563e-02,  6.6912e-02,\n",
      "         -6.3668e-03,  8.4107e-02,  1.7071e-01,  4.4009e-02,  1.8215e-02,\n",
      "          6.3900e-02,  5.6475e-02,  2.0577e-03,  6.4332e-02, -1.5987e-02,\n",
      "          3.2494e-02,  4.0789e-02, -1.7265e-02,  3.1392e-02, -4.5062e-02,\n",
      "         -6.9425e-02,  3.0897e-02,  2.0968e-02,  3.5488e-02,  8.9162e-02,\n",
      "         -1.3000e-01, -5.6480e-02, -1.3870e-02, -3.7643e-02,  1.8146e-02,\n",
      "         -2.0239e-02,  1.2286e-01,  1.0229e-01,  9.4493e-02, -2.1190e-02,\n",
      "          7.0476e-02, -1.4266e-04,  4.9761e-02, -2.6066e-02,  1.2415e-03,\n",
      "         -2.0719e-02, -4.8260e-02,  1.0160e-02,  2.4870e-02,  7.4811e-02,\n",
      "         -1.0488e-02, -8.0457e-02, -1.5282e-02, -2.3008e-02, -7.8217e-02,\n",
      "         -6.9087e-02, -5.2520e-02,  1.2460e-02,  9.5719e-03, -3.5082e-02,\n",
      "         -1.0009e-01, -1.0996e-01, -2.3959e-02,  2.5785e-02, -3.5853e-04,\n",
      "         -2.0642e-02, -5.0871e-02, -2.9130e-02, -5.6157e-02,  3.2909e-02,\n",
      "         -1.6210e-02,  9.8381e-03,  3.6057e-02, -8.2764e-02, -1.2433e-02,\n",
      "          7.1118e-03, -3.2684e-02, -9.4528e-02, -4.6649e-02,  1.7469e-02,\n",
      "         -4.7731e-03, -3.4192e-02,  1.0016e-02,  3.1786e-02,  6.0582e-02,\n",
      "          6.9984e-02,  7.0542e-03, -1.0460e-01,  3.3910e-02, -3.1551e-02,\n",
      "          4.1591e-02,  8.6932e-02, -6.9838e-02,  2.6072e-02, -3.5156e-02,\n",
      "         -6.8120e-02, -7.1643e-02, -7.3969e-02,  4.0712e-04, -9.1669e-03,\n",
      "         -4.6956e-02, -1.5336e-02, -3.3788e-02,  2.2551e-03,  1.7358e-02,\n",
      "         -1.8591e-02, -4.2544e-02, -9.7673e-02,  1.3634e-01,  3.4670e-02,\n",
      "         -1.2499e-02,  1.0257e-01,  2.8475e-03, -2.7471e-02, -7.6454e-02,\n",
      "         -1.4923e-02,  2.3857e-02,  6.6666e-02, -2.0400e-02, -1.4440e-02,\n",
      "          1.2386e-01,  6.9217e-03, -2.8583e-02,  1.7779e-02,  3.5083e-01,\n",
      "         -3.8919e-01,  4.5305e-02,  1.0371e-01, -5.6647e-03,  1.0559e-01,\n",
      "         -3.9454e-02,  7.0810e-02,  6.2368e-02,  1.1877e-01,  1.5406e-01,\n",
      "         -3.1121e-02, -2.1252e-02, -9.6277e-02,  6.7318e-02,  1.0553e-02,\n",
      "          2.0848e-02,  5.0982e-02, -7.1828e-02,  3.1063e-03,  2.0483e-02,\n",
      "         -6.2537e-02, -1.0284e-02, -2.3263e-02, -7.1690e-02,  1.3969e-02,\n",
      "          2.7773e-02,  3.5212e-02, -3.7041e-02,  3.4511e-03, -2.4919e-02,\n",
      "          5.6154e-02, -4.2725e-02, -5.5340e-03, -1.0701e-01,  8.4178e-02,\n",
      "         -3.9239e-02, -7.9305e-02,  1.5113e-01,  9.1773e-03,  4.1365e-02,\n",
      "          6.2559e-02, -6.2119e-02,  1.4988e-02,  1.4421e-02, -1.1347e-02,\n",
      "          1.7729e-02,  6.4873e-02,  2.1972e-02, -2.3624e-02,  1.6013e-01,\n",
      "         -2.3654e-02,  5.4127e-02, -1.0577e-01,  7.8154e-02,  1.0887e-01,\n",
      "         -6.0795e-02,  2.7255e-02, -6.8780e-03,  8.2754e-02,  4.3499e-02,\n",
      "         -5.6289e-02, -6.9130e-02, -4.5399e-02, -1.0185e-01,  9.8976e-02,\n",
      "          2.2533e-02,  1.6057e-02, -1.2166e-01,  1.2990e-02, -4.1795e-02,\n",
      "          2.7845e-02, -3.0098e-03, -6.5929e-02,  9.3089e-02, -4.8920e-02,\n",
      "          1.6414e-02,  1.9934e-02, -4.9946e-02, -2.2188e-03, -9.7081e-02,\n",
      "          1.3230e-03,  5.9366e-02,  7.4685e-02, -6.6880e-02,  7.6049e-02,\n",
      "         -5.3841e-02,  3.7525e-02, -2.4051e-02, -6.0837e-02, -4.9455e-02,\n",
      "         -1.2835e-01,  7.1273e-02,  8.2448e-03, -2.4222e-02, -2.0194e-02,\n",
      "          1.4828e-02, -8.4307e-04, -9.3173e-02, -8.8207e-02, -2.7742e-02,\n",
      "         -7.9302e-02,  7.2562e-02, -6.1378e-02, -4.1767e-02, -3.8987e-02,\n",
      "         -2.9905e-02,  1.2688e-02,  3.7553e-03, -1.1192e-02, -1.2206e-01,\n",
      "         -3.2982e-02, -7.6435e-02,  6.5688e-02,  2.6813e-02, -1.1179e-02,\n",
      "         -4.9633e-02,  6.1542e-02,  3.0834e-02, -1.0269e-02,  6.4870e-02,\n",
      "         -5.7638e-02,  4.3060e-03, -1.0008e-01, -3.5091e-01,  6.5391e-02,\n",
      "          1.0947e-02,  3.4814e-02,  3.4923e-02, -9.1231e-02,  3.7350e-02,\n",
      "          2.6930e-02,  5.2713e-05,  8.5150e-02, -6.1180e-02,  2.9461e-02,\n",
      "         -3.6984e-02, -8.5079e-02,  2.6530e-02, -3.3637e-02, -5.4362e-02,\n",
      "          5.5838e-02, -3.9802e-02, -2.1221e-02, -1.4457e-01,  1.7748e-02,\n",
      "         -4.1487e-02, -1.0857e-02,  9.1439e-03, -2.6833e-03, -6.3365e-02,\n",
      "         -7.0894e-02,  2.3556e-02,  8.8968e-02,  2.6706e-02, -1.0208e-01,\n",
      "         -5.1768e-02,  2.8664e-02,  6.1897e-02,  1.1344e-01, -5.2443e-02,\n",
      "         -3.8825e-03, -9.8364e-02,  8.7298e-02,  8.0411e-02,  2.0813e-01,\n",
      "         -3.0991e-02,  1.8852e-01,  8.7562e-03,  1.1540e-01,  6.2689e-02,\n",
      "         -2.1062e-02, -8.3236e-02, -4.2962e-02, -3.3596e-02, -4.5224e-02,\n",
      "          4.4709e-02, -6.4868e-02, -4.5981e-02,  4.6211e-02,  3.0328e-03,\n",
      "         -6.5393e-03, -1.5591e-02,  1.8842e-01,  2.3078e-03,  7.2521e-03,\n",
      "         -2.7918e-02, -1.3538e-02, -6.2095e-03,  8.3305e-05, -5.9651e-03,\n",
      "         -3.7315e-02,  1.7645e-02, -5.0366e-02, -4.9478e-03, -5.8123e-02,\n",
      "         -1.8941e-02, -1.6458e-02,  2.2910e-02,  1.1502e-01, -2.0251e-02,\n",
      "          3.1692e-02, -3.7045e-02,  1.0548e-01,  3.3315e-02, -2.5456e-02,\n",
      "          5.1958e-02,  1.1188e-01,  1.2742e-02,  1.0706e-02, -2.4340e-02,\n",
      "         -2.9807e-02,  1.5678e-02,  1.3007e-01, -4.3743e-02,  1.0692e-01,\n",
      "          8.7125e-02,  5.6808e-03, -9.4614e-02,  4.2949e-02,  1.5396e-02,\n",
      "          5.0958e-02, -6.4023e-01, -3.6584e-02,  1.3790e-01, -5.4191e-03,\n",
      "         -2.2834e-02,  6.8948e-02,  3.4192e-02, -8.6519e-02, -9.7056e-03,\n",
      "         -4.6012e-02,  7.0841e-02,  5.2503e-02,  6.9356e-02, -5.9406e-02,\n",
      "         -2.7379e-02,  8.0969e-02, -5.4704e-02, -1.7284e-03,  5.3504e-02,\n",
      "         -2.6389e-01, -4.4178e-03, -5.2920e-02,  1.0923e-01, -5.2700e-02,\n",
      "          9.0411e-02,  8.1851e-02, -7.8554e-02,  3.2444e-02,  4.9755e-02,\n",
      "          4.3072e-02,  5.0764e-02,  3.7505e-02, -6.3760e-02,  5.7162e-02,\n",
      "          1.1650e-01,  7.0077e-02,  4.1648e-02,  1.1792e+01,  2.6194e-02,\n",
      "          5.4933e-02,  3.7338e-03,  5.0105e-02, -6.1551e-02,  4.3107e-03,\n",
      "         -1.3176e-01, -1.2037e-02,  1.6352e-01, -2.9754e-02, -6.3834e-02,\n",
      "         -5.8230e-02, -1.0544e-01,  3.1869e-02,  6.2311e-03, -4.2405e-02,\n",
      "         -4.6961e-02,  5.2618e-02, -8.9570e-02, -5.9202e-02,  1.1712e-02,\n",
      "          8.3972e-02,  2.3883e-02, -2.3082e-02,  2.9189e-02,  7.6207e-02,\n",
      "         -1.0634e-01, -2.9694e-02,  7.7876e-02,  9.6785e-03,  1.1147e-02,\n",
      "          4.7138e-02,  1.6069e-02,  1.3129e-01,  3.6985e-02, -1.9742e-02,\n",
      "          6.0056e-02, -1.1609e-02,  3.4008e-02,  3.5811e-02, -1.9208e-02,\n",
      "          1.0374e-01,  3.6435e-02,  4.5250e-02,  5.9725e-03,  4.0341e-03,\n",
      "          1.5096e-01,  4.5603e-02,  6.4334e-02,  1.1201e-01, -4.6618e-02,\n",
      "          1.3388e-01,  2.9168e-02,  2.5302e-03,  2.5713e-02,  2.9211e-02,\n",
      "         -1.4521e-02,  9.1087e-02,  5.0253e-02, -3.9415e-02,  6.7097e-02,\n",
      "         -6.5439e-03,  3.5600e-02, -1.2213e-02,  1.0045e-01,  1.3769e-01,\n",
      "          6.0418e-02, -1.0507e-01, -6.9069e-02, -6.7938e-03, -5.1517e-02,\n",
      "         -4.5078e-02, -1.1527e-02,  8.1811e-02, -5.7189e-02, -4.4112e-03,\n",
      "         -3.5223e-02, -1.0305e-02, -2.2172e-02, -1.6422e-02,  1.5389e-02,\n",
      "         -2.1827e-03, -4.5947e-02,  8.8067e-02,  5.5791e-02,  2.7615e-02,\n",
      "          7.2094e-02, -4.6943e-02, -1.1526e-01, -9.8929e-03,  5.1136e-02,\n",
      "         -3.5075e-02, -3.8651e-02,  5.1887e-02, -9.0843e-02,  1.0622e-01,\n",
      "         -1.4955e-01,  3.3512e-02,  1.0368e-01, -1.0890e-01,  2.4778e-02,\n",
      "         -2.4435e-02,  6.0282e-02,  2.6010e-02,  3.5580e-02, -4.4499e-02,\n",
      "         -1.6905e-02,  3.7836e-02, -1.1840e-02, -3.5667e-02,  1.0957e-02,\n",
      "          4.4288e-02, -5.1330e-02,  4.1176e-02,  3.1323e-02, -7.8614e-03,\n",
      "          2.0006e-02,  2.4279e-02,  8.9774e-02, -7.5258e-02, -2.9248e-02,\n",
      "         -6.0289e-02, -1.6029e-02, -4.1149e-02, -2.1787e-02, -1.8895e-02,\n",
      "          6.4288e-02, -3.0288e-02,  6.4764e-02, -8.8052e-03,  1.9616e-02,\n",
      "          7.1916e-02, -7.3103e-02,  7.4203e-02, -6.2869e-02, -4.3141e-02,\n",
      "          4.4882e-02, -1.1754e-02,  6.3197e-02, -1.8505e-02, -9.1761e-02,\n",
      "         -5.1052e-02, -9.6919e-02,  1.3797e-02,  8.5993e-02,  6.6709e-02,\n",
      "          7.3238e-02,  6.6428e-02, -8.6125e-02, -7.8206e-02,  4.3027e-02,\n",
      "          6.9463e-02,  6.3709e-03,  1.1772e-02, -5.7913e-02, -5.3001e-02,\n",
      "          1.0727e-01, -5.5523e-03, -3.1501e-02,  2.5130e-02,  4.9741e-02,\n",
      "          9.8048e-02,  3.5870e-02,  9.9608e-02,  2.4971e-02,  1.6551e-02,\n",
      "          1.3239e-02,  3.8363e-02, -4.5335e-02,  3.9748e-02,  1.3632e-02,\n",
      "         -1.1738e-01, -7.9431e-02,  4.5755e-02,  9.3624e-02,  7.1120e-03,\n",
      "         -1.1123e-01, -1.1741e-02, -3.8916e-02]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "try_model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the input text\n",
    "text = \"This is an example sentence.\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Feed the input into the model\n",
    "output = try_model(**encoded_input)\n",
    "\n",
    "# Extract the CLS token embedding\n",
    "cls_embedding = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(cls_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "# model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer, RobertaModelWithHeads\n",
    "# from transformers.adapters.composition import Stack\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "# model_name = \"roberta-base\"\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "# model = RobertaModelWithHeads.from_pretrained(model_name)\n",
    "\n",
    "# # Add a new adapter\n",
    "# adapter_name = \"my_adapter\"\n",
    "# model.add_adapter(adapter_name)\n",
    "\n",
    "# # Add a classification head for your task\n",
    "# num_labels = 2  # Change this value according to your task\n",
    "# model.add_classification_head(adapter_name, num_labels=num_labels)\n",
    "\n",
    "# # Set the adapters to be used in training\n",
    "# model.train_adapter([adapter_name])\n",
    "\n",
    "# # You can now train your model with your dataset and use it for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This movie was amazing!\"\n",
    "# cls = model(text)\n",
    "# print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from load_wiki_dataset import wikiData\n",
    "from RoBERTa import CustomRobertaModel\n",
    "from losses import align_loss, uniform_loss\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')\n",
    "    parser.add_argument('--data_path', type=str, default='../data/wiki1m_for_simcse.txt', help='Path to the dataset')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def train_wiki(args,model):\n",
    "    # Load dataset\n",
    "    with open(args.data_path, 'r', encoding='UTF-8') as f:\n",
    "        input_text = f.readlines()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # ADD THIS LINE\n",
    "\n",
    "    wiki = wikiData(input_text, tokenizer)  # PASS tokenizer TO wikiData\n",
    "    train_params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    trainloader = DataLoader(wiki, **train_params)\n",
    "    \n",
    "    # Initialize model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    # Training loop\n",
    "    # for epoch in range(args.epochs):\n",
    "    #     epoch_loss = 0\n",
    "    #     for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "    #         optimizer.zero_grad()\n",
    "    #         batch=tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    #         batch = [torch.tensor(item).to(device) for item in batch]  # Move batch to device (GPU), change it according to dataset\n",
    "    #         loss, _ = model(batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         epoch_loss += loss.item()\n",
    "\n",
    "    #     print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    for epoch in range(args.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}  # move batch to device\n",
    "                \n",
    "                loss, _ = model(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                # print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    torch.save(model.state_dict(), f'our_loss_{epoch + 1}wiki_model.pth')\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "raw_model= CustomRobertaModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125000/125000 [5:39:37<00:00,  6.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.7516470831336677\n"
     ]
    }
   ],
   "source": [
    "simcse_1wiki_model = train_wiki(args, raw_model)\n",
    "    # Save the trained model if needed\n",
    "simcse_1wiki_model.save_pretrained('./simcse_1wiki_model')\n",
    "torch.save(simcse_1wiki_model.state_dict(), 'simcse_1wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RoBERTa import CustomRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model2 = CustomRobertaModel()\n",
    "wiki_model2.load_state_dict(torch.load('1wiki.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./trained_model_wiki were not used when initializing RobertaModel: ['roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.value.bias', 'roberta.roberta.encoder.layer.4.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.attention.self.query.weight', 'roberta.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.output.dense.weight', 'roberta.roberta.encoder.layer.8.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.roberta.encoder.layer.11.output.dense.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.key.weight', 'roberta.roberta.encoder.layer.1.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.attention.self.key.bias', 'roberta.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.query.weight', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.7.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.query.weight', 'roberta.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.output.dense.weight', 'roberta.roberta.encoder.layer.9.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.5.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.4.attention.self.query.bias', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.10.attention.self.query.weight', 'roberta.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.8.attention.self.value.bias', 'roberta.roberta.encoder.layer.8.attention.output.dense.weight', 'roberta_m.roberta.embeddings.position_embeddings.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.query.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.query.bias', 'roberta.roberta.encoder.layer.11.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.2.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.roberta.embeddings.LayerNorm.weight', 'roberta.roberta.encoder.layer.8.attention.self.query.bias', 'roberta.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.2.output.dense.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.attention.self.key.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.3.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.embeddings.token_type_embeddings.weight', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.11.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.3.output.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.1.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.8.output.dense.weight', 'roberta_m.roberta.encoder.layer.11.attention.self.value.bias', 'roberta.roberta.encoder.layer.3.attention.self.query.bias', 'roberta.roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.roberta.encoder.layer.3.output.dense.bias', 'roberta.roberta.encoder.layer.11.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.7.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.key.weight', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.output.dense.bias', 'roberta_m.roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.value.weight', 'roberta.roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.5.attention.self.value.weight', 'roberta.roberta.encoder.layer.6.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.10.output.dense.weight', 'roberta_m.roberta.encoder.layer.7.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.value.bias', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.value.weight', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.query.bias', 'roberta.roberta.encoder.layer.5.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.value.weight', 'roberta.roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.intermediate.dense.weight', 'roberta_m.roberta.embeddings.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.output.dense.weight', 'roberta.roberta.encoder.layer.3.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.key.weight', 'roberta.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.attention.self.key.weight', 'roberta.roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.roberta.encoder.layer.6.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.4.attention.self.key.bias', 'roberta.roberta.encoder.layer.2.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.0.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.attention.self.query.bias', 'roberta.roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.roberta.encoder.layer.3.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.6.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.value.weight', 'roberta.roberta.encoder.layer.0.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.attention.self.value.bias', 'roberta.roberta.encoder.layer.1.attention.self.key.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.key.weight', 'roberta.roberta.encoder.layer.2.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.roberta.pooler.dense.bias', 'roberta_m.roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.roberta.encoder.layer.11.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.0.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.key.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.attention.self.query.weight', 'roberta.roberta.embeddings.token_type_embeddings.weight', 'roberta.roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.8.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.9.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.value.bias', 'roberta.roberta.pooler.dense.weight', 'roberta.roberta.encoder.layer.2.attention.self.query.weight', 'roberta.roberta.encoder.layer.6.output.dense.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.8.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.roberta.encoder.layer.10.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.11.output.dense.weight', 'roberta.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.query.weight', 'roberta.roberta.encoder.layer.7.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.2.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.6.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.3.attention.self.value.weight', 'roberta.roberta.encoder.layer.5.attention.self.value.bias', 'roberta.roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.roberta.encoder.layer.11.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta_m.roberta.embeddings.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.roberta.encoder.layer.10.output.dense.bias', 'roberta.roberta.encoder.layer.2.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.10.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.value.bias', 'roberta.roberta.encoder.layer.10.attention.self.key.bias', 'roberta.roberta.encoder.layer.10.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.2.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.key.weight', 'roberta.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.output.dense.bias', 'roberta.roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.5.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.5.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.5.output.dense.bias', 'roberta.roberta.encoder.layer.6.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.key.bias', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.8.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.0.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.1.output.dense.bias', 'roberta.roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.value.bias', 'roberta.roberta.encoder.layer.6.attention.self.query.weight', 'roberta.roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.roberta.encoder.layer.4.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.query.weight', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.6.output.dense.weight', 'roberta.roberta.encoder.layer.7.attention.self.value.bias', 'roberta.roberta.encoder.layer.0.attention.self.key.weight', 'roberta.roberta.encoder.layer.0.attention.self.query.bias', 'roberta.roberta.encoder.layer.6.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.dense.bias', 'roberta_m.roberta.pooler.dense.weight', 'roberta.roberta.encoder.layer.11.attention.self.key.bias', 'roberta.roberta.encoder.layer.2.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.key.weight', 'roberta.roberta.encoder.layer.5.output.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.value.bias', 'roberta.roberta.encoder.layer.1.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.4.output.dense.weight', 'roberta.roberta.encoder.layer.8.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.8.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.key.bias', 'roberta.roberta.embeddings.position_ids', 'roberta_m.roberta.encoder.layer.11.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.query.weight', 'roberta.roberta.encoder.layer.1.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.value.bias', 'roberta.roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.attention.self.query.bias', 'roberta.roberta.encoder.layer.9.output.dense.bias', 'roberta_m.roberta.encoder.layer.0.output.dense.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.value.weight', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.4.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.roberta.embeddings.word_embeddings.weight', 'roberta_m.roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.3.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.value.weight', 'roberta.roberta.embeddings.LayerNorm.bias', 'roberta.roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.11.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.roberta.encoder.layer.7.output.dense.bias', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.key.bias', 'roberta.roberta.encoder.layer.0.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.attention.self.value.bias', 'roberta.roberta.encoder.layer.4.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.roberta.encoder.layer.6.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.6.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.embeddings.word_embeddings.weight', 'roberta_m.roberta.encoder.layer.2.attention.self.key.bias', 'roberta.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.10.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.1.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.4.output.dense.bias', 'roberta.roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.key.weight', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.query.bias', 'roberta.roberta.encoder.layer.3.attention.self.key.bias', 'roberta.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.3.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.2.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.3.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.1.output.dense.weight', 'roberta_m.roberta.embeddings.position_ids', 'roberta_m.roberta.encoder.layer.10.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.query.bias', 'roberta.roberta.encoder.layer.1.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.key.bias', 'roberta.roberta.embeddings.position_embeddings.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.4.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.roberta.encoder.layer.7.attention.self.value.weight', 'roberta.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.1.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.6.output.dense.bias', 'roberta.roberta.encoder.layer.6.attention.self.value.bias', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.0.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.1.intermediate.dense.weight', 'roberta_m.roberta.pooler.dense.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.query.bias', 'roberta.roberta.encoder.layer.8.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.value.bias', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.5.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.10.attention.self.value.weight', 'roberta.roberta.encoder.layer.5.attention.self.query.weight', 'roberta.roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.0.output.dense.bias', 'roberta.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.roberta.encoder.layer.4.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.query.weight', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.11.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.query.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./trained_model_wiki and are newly initialized: ['roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,RobertaTokenizer # Load the trained model \n",
    "\n",
    "wiki_model = AutoModel.from_pretrained('./trained_model_wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_allsides_dataset import allsidesData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import argparse\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from load_wiki_dataset import wikiData\n",
    "from losses import align_loss, uniform_loss\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "def train_allsides(args, model_to_train):\n",
    "    # Load dataset\n",
    "    with open(args.data_path, 'r', encoding='UTF-8') as f:\n",
    "        input_text = f.readlines()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # ADD THIS LINE\n",
    "\n",
    "    allsides = allsidesData(input_text, tokenizer)  # PASS tokenizer TO wikiData\n",
    "    train_params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    trainloader = DataLoader(allsides, **train_params)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = model_to_train\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    # Training loop\n",
    "    # for epoch in range(args.epochs):\n",
    "    #     epoch_loss = 0\n",
    "    #     for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "    #         optimizer.zero_grad()\n",
    "    #         batch=tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    #         batch = [torch.tensor(item).to(device) for item in batch]  # Move batch to device (GPU), change it according to dataset\n",
    "    #         loss, _ = model(batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         epoch_loss += loss.item()\n",
    "\n",
    "    #     print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    for epoch in range(args.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}  # move batch to device\n",
    "                # print(model(batch))\n",
    "                loss, cls = model(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                # print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/allsides.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 65720/65720 [3:01:13<00:00,  6.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.609909778521399\n"
     ]
    }
   ],
   "source": [
    "allsides_wiki_model = train_allsides(args,wiki_model2)\n",
    "    # Save the trained model if needed\n",
    "allsides_wiki_model.save_pretrained('./allsides_wiki_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(allsides_wiki_model.state_dict(), 'allsides_wiki_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  70%|██████▉   | 86938/125000 [3:55:10<1:42:57,  6.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m allsides_wiki2_model \u001b[39m=\u001b[39m train_wiki(args,allsides_wiki_model)\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Save the trained model if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m allsides_wiki2_model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m./allsides_wiki2_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 61\u001b[0m, in \u001b[0;36mtrain_wiki\u001b[1;34m(args, model)\u001b[0m\n\u001b[0;32m     59\u001b[0m             loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 61\u001b[0m             epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     62\u001b[0m             \u001b[39m# print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(trainloader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "allsides_wiki2_model = train_wiki(args,allsides_wiki_model)\n",
    "    # Save the trained model if needed\n",
    "allsides_wiki2_model.save_pretrained('./allsides_wiki2_model')\n",
    "torch.save(allsides_wiki2_model.state_dict(), 'allsides_wiki2_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "wiki_model = CustomRobertaModel()\n",
    "wiki_model.load_state_dict(torch.load('1wiki.pth'))\n",
    "allsides_wiki_model = CustomRobertaModel()\n",
    "allsides_wiki_model.load_state_dict(torch.load('allsides_wiki_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "simcse_1wiki_model = CustomRobertaModel()\n",
    "simcse_1wiki_model.load_state_dict(torch.load('simcse_1wiki_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125000/125000 [5:35:28<00:00,  6.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.8066049778327944\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'allsides_wiki2_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Save the trained model if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m simcse_1wiki_model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m./simcse_2wiki_model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39msave(allsides_wiki2_model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39msimcse_2wiki_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'allsides_wiki2_model' is not defined"
     ]
    }
   ],
   "source": [
    "simcse_2wiki_model = train_wiki(args,simcse_1wiki_model)\n",
    "    # Save the trained model if needed\n",
    "simcse_2wiki_model.save_pretrained('./simcse_2wiki_model')\n",
    "torch.save(simcse_2wiki_model.state_dict(), 'simcse_2wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_2wiki_model.save_pretrained('./simcse_2wiki_model')\n",
    "torch.save(simcse_2wiki_model.state_dict(), 'simcse_2wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomRobertaModel:\n\tUnexpected key(s) in state_dict: \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRoBERTa\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomRobertaModel\n\u001b[0;32m      2\u001b[0m simcse_2wiki_model \u001b[39m=\u001b[39m CustomRobertaModel()\n\u001b[1;32m----> 3\u001b[0m simcse_2wiki_model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39msimcse_2wiki_model.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomRobertaModel:\n\tUnexpected key(s) in state_dict: \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\", \"roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\". "
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "simcse_2wiki_model = CustomRobertaModel()\n",
    "simcse_2wiki_model.load_state_dict(torch.load('simcse_2wiki_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125000/125000 [5:38:20<00:00,  6.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.821558924482346\n"
     ]
    }
   ],
   "source": [
    "simcse_3wiki_model = train_wiki(args,simcse_2wiki_model)\n",
    "    # Save the trained model if needed\n",
    "simcse_3wiki_model.save_pretrained('./simcse_3wiki_model')\n",
    "torch.save(simcse_3wiki_model.state_dict(), 'simcse_3wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "real_simcse_wiki_model = CustomRobertaModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 2\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 125000/125000 [8:50:44<00:00,  3.93it/s]   \n",
      "Epoch 2/2: 100%|██████████| 125000/125000 [8:40:10<00:00,  4.01it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5896223703017234\n"
     ]
    }
   ],
   "source": [
    "real_simcse_2wiki_model  = train_wiki(args, real_simcse_wiki_model)\n",
    "torch.save(real_simcse_2wiki_model.state_dict(), 'real_simcse_2wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1: 100%|██████████| 125000/125000 [8:13:29<00:00,  4.22it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.6381619698782004\n"
     ]
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "real_simcse_wiki_model = CustomRobertaModel()\n",
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\"\n",
    "real_simcse_1wiki_model  = train_wiki(args, real_simcse_wiki_model)\n",
    "torch.save(real_simcse_1wiki_model.state_dict(), 'real_simcse_1wiki_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_moreal_simcse_wiki_model = CustomRobertaModel()\n",
    "try_moreal_simcse_wiki_model.load_state_dict(torch.load('real_simcse_2wiki_model.pth'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdf77093bbdbf1309a6eb466c68765f3f88bd63ff87e3bfe863ca62958fcd8bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
