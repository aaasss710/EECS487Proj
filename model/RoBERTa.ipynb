{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from RoBERTa import *\n",
    "# model = CustomRobertaModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# input_tokens =tokenizer(\"test and test.\", return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "# input_tokens = input_tokens.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0255e-01,  1.0369e-01, -1.0960e-02, -5.4950e-02,  1.2176e-01,\n",
      "         -3.2302e-03, -2.3434e-02,  4.3359e-02,  5.0298e-02, -8.6572e-02,\n",
      "         -2.3857e-02,  3.4339e-02,  4.3145e-02, -7.7734e-02,  5.4695e-02,\n",
      "         -1.0053e-01, -4.0279e-02, -5.7762e-03, -1.4725e-02, -4.9593e-02,\n",
      "         -1.3021e-01,  2.5552e-02,  2.0522e-02,  1.0200e-01, -6.3678e-02,\n",
      "          8.8177e-02,  1.4348e-01,  5.3061e-02, -7.1654e-02,  5.3694e-02,\n",
      "         -6.8900e-02, -9.9970e-02,  8.2682e-02, -1.3511e-02,  2.0117e-02,\n",
      "          1.3400e-01,  2.7797e-03, -8.3667e-04, -5.8601e-02,  5.1972e-02,\n",
      "          1.2828e-02,  2.1757e-01,  1.7009e-02, -4.6349e-02,  6.8069e-02,\n",
      "          3.1592e-02,  3.6040e-02, -2.5366e-02,  1.2174e-02, -1.6848e-02,\n",
      "          4.5981e-03,  5.4572e-02, -2.1784e-02,  5.7348e-02, -1.3289e-01,\n",
      "          6.8254e-02,  3.0545e-02,  7.2083e-02,  8.2089e-02, -1.2538e-01,\n",
      "         -3.0586e-02, -1.8470e-01, -6.9079e-02, -6.0674e-02,  5.5125e-02,\n",
      "         -1.0963e-01, -3.8253e-02,  5.7974e-02,  2.5209e-02,  5.6676e-02,\n",
      "          6.6650e-02, -5.7630e-02, -7.0528e-03,  1.0601e-02,  3.2166e-02,\n",
      "         -4.5240e-03, -1.8739e-02,  5.8682e-01, -2.7962e-02,  1.2938e-02,\n",
      "          6.2185e-02, -8.3002e-02,  5.8404e-01,  5.5627e-02, -4.2669e-03,\n",
      "         -5.6232e-02,  1.4140e-01, -5.6821e-03,  4.3118e-02,  7.9804e-02,\n",
      "         -4.8317e-02,  7.8521e-02, -4.1143e-02,  1.0308e-02,  5.1249e-02,\n",
      "          3.2183e-03, -2.3498e-02,  1.2585e-01, -9.8542e-02, -7.0455e-02,\n",
      "         -2.6493e-02, -6.5842e-02,  1.0553e-01,  8.8572e-02, -2.0663e-02,\n",
      "         -7.9825e-03,  9.0078e-02, -3.3579e-02,  4.0537e-02, -4.6728e-02,\n",
      "         -2.1361e-02, -2.6896e-03,  6.8196e-02, -2.1516e-02,  3.5645e-02,\n",
      "         -2.5768e-02,  1.9392e-02,  8.0571e-02,  5.7537e-02,  3.3482e-02,\n",
      "         -3.4843e-02,  6.3484e-02,  9.8376e-02, -6.1567e-02, -7.0961e-02,\n",
      "         -1.0034e-01, -5.0497e-02, -9.9992e-03,  7.0938e-03,  1.6755e-02,\n",
      "         -4.6749e-02, -1.7698e-01,  9.0778e-04,  8.4737e-02,  1.5545e-02,\n",
      "          1.1887e-02,  2.6032e-02, -4.9570e-02,  2.0886e-02, -9.2508e-02,\n",
      "         -1.9673e-03,  3.5068e-02,  5.0853e-02,  1.3869e-02,  1.1944e-01,\n",
      "          8.0163e-02, -5.3810e-02, -2.0149e-02,  4.8207e-02,  5.3597e-03,\n",
      "          1.2693e-01, -5.9935e-02, -3.7519e-02, -3.4062e-02, -5.1985e-02,\n",
      "          5.3274e-01,  1.1878e-01,  1.5434e-01,  1.7108e-02,  1.4908e-02,\n",
      "          2.1055e-01,  6.0076e-02,  3.1167e-02, -3.8785e-02, -9.2120e-03,\n",
      "         -1.6625e-02, -6.8709e-02, -7.0710e-02,  3.8311e-02,  1.9385e-02,\n",
      "          6.8637e-02,  3.7470e-02,  7.3401e-03, -1.3814e-02, -1.1375e-01,\n",
      "         -6.1690e-02,  6.5720e-02,  4.1201e-03, -1.1441e-01,  3.9362e-02,\n",
      "          8.4953e-03,  1.0701e-01, -1.1664e-01,  5.6977e-03, -2.3933e-02,\n",
      "          6.8238e-02,  2.4265e-02, -1.0737e-02, -3.4237e-02,  7.3842e-02,\n",
      "          1.0543e-01, -3.5569e-02, -3.6212e-02, -2.5952e-02, -2.9731e-02,\n",
      "          8.8929e-02, -8.2062e-03, -3.3984e-03,  2.7404e-02, -5.5331e-02,\n",
      "          8.1087e-03, -6.8448e-02,  1.1682e-01, -1.1967e-01,  2.9929e-02,\n",
      "         -2.3187e-02,  3.2698e-03,  7.8010e-02,  7.1490e-04, -9.3949e-02,\n",
      "          9.1565e-03,  9.6677e-03, -3.8123e-02,  7.9563e-02,  6.6912e-02,\n",
      "         -6.3668e-03,  8.4107e-02,  1.7071e-01,  4.4009e-02,  1.8215e-02,\n",
      "          6.3900e-02,  5.6475e-02,  2.0577e-03,  6.4332e-02, -1.5987e-02,\n",
      "          3.2494e-02,  4.0789e-02, -1.7265e-02,  3.1392e-02, -4.5062e-02,\n",
      "         -6.9425e-02,  3.0897e-02,  2.0968e-02,  3.5488e-02,  8.9162e-02,\n",
      "         -1.3000e-01, -5.6480e-02, -1.3870e-02, -3.7643e-02,  1.8146e-02,\n",
      "         -2.0239e-02,  1.2286e-01,  1.0229e-01,  9.4493e-02, -2.1190e-02,\n",
      "          7.0476e-02, -1.4266e-04,  4.9761e-02, -2.6066e-02,  1.2415e-03,\n",
      "         -2.0719e-02, -4.8260e-02,  1.0160e-02,  2.4870e-02,  7.4811e-02,\n",
      "         -1.0488e-02, -8.0457e-02, -1.5282e-02, -2.3008e-02, -7.8217e-02,\n",
      "         -6.9087e-02, -5.2520e-02,  1.2460e-02,  9.5719e-03, -3.5082e-02,\n",
      "         -1.0009e-01, -1.0996e-01, -2.3959e-02,  2.5785e-02, -3.5853e-04,\n",
      "         -2.0642e-02, -5.0871e-02, -2.9130e-02, -5.6157e-02,  3.2909e-02,\n",
      "         -1.6210e-02,  9.8381e-03,  3.6057e-02, -8.2764e-02, -1.2433e-02,\n",
      "          7.1118e-03, -3.2684e-02, -9.4528e-02, -4.6649e-02,  1.7469e-02,\n",
      "         -4.7731e-03, -3.4192e-02,  1.0016e-02,  3.1786e-02,  6.0582e-02,\n",
      "          6.9984e-02,  7.0542e-03, -1.0460e-01,  3.3910e-02, -3.1551e-02,\n",
      "          4.1591e-02,  8.6932e-02, -6.9838e-02,  2.6072e-02, -3.5156e-02,\n",
      "         -6.8120e-02, -7.1643e-02, -7.3969e-02,  4.0712e-04, -9.1669e-03,\n",
      "         -4.6956e-02, -1.5336e-02, -3.3788e-02,  2.2551e-03,  1.7358e-02,\n",
      "         -1.8591e-02, -4.2544e-02, -9.7673e-02,  1.3634e-01,  3.4670e-02,\n",
      "         -1.2499e-02,  1.0257e-01,  2.8475e-03, -2.7471e-02, -7.6454e-02,\n",
      "         -1.4923e-02,  2.3857e-02,  6.6666e-02, -2.0400e-02, -1.4440e-02,\n",
      "          1.2386e-01,  6.9217e-03, -2.8583e-02,  1.7779e-02,  3.5083e-01,\n",
      "         -3.8919e-01,  4.5305e-02,  1.0371e-01, -5.6647e-03,  1.0559e-01,\n",
      "         -3.9454e-02,  7.0810e-02,  6.2368e-02,  1.1877e-01,  1.5406e-01,\n",
      "         -3.1121e-02, -2.1252e-02, -9.6277e-02,  6.7318e-02,  1.0553e-02,\n",
      "          2.0848e-02,  5.0982e-02, -7.1828e-02,  3.1063e-03,  2.0483e-02,\n",
      "         -6.2537e-02, -1.0284e-02, -2.3263e-02, -7.1690e-02,  1.3969e-02,\n",
      "          2.7773e-02,  3.5212e-02, -3.7041e-02,  3.4511e-03, -2.4919e-02,\n",
      "          5.6154e-02, -4.2725e-02, -5.5340e-03, -1.0701e-01,  8.4178e-02,\n",
      "         -3.9239e-02, -7.9305e-02,  1.5113e-01,  9.1773e-03,  4.1365e-02,\n",
      "          6.2559e-02, -6.2119e-02,  1.4988e-02,  1.4421e-02, -1.1347e-02,\n",
      "          1.7729e-02,  6.4873e-02,  2.1972e-02, -2.3624e-02,  1.6013e-01,\n",
      "         -2.3654e-02,  5.4127e-02, -1.0577e-01,  7.8154e-02,  1.0887e-01,\n",
      "         -6.0795e-02,  2.7255e-02, -6.8780e-03,  8.2754e-02,  4.3499e-02,\n",
      "         -5.6289e-02, -6.9130e-02, -4.5399e-02, -1.0185e-01,  9.8976e-02,\n",
      "          2.2533e-02,  1.6057e-02, -1.2166e-01,  1.2990e-02, -4.1795e-02,\n",
      "          2.7845e-02, -3.0098e-03, -6.5929e-02,  9.3089e-02, -4.8920e-02,\n",
      "          1.6414e-02,  1.9934e-02, -4.9946e-02, -2.2188e-03, -9.7081e-02,\n",
      "          1.3230e-03,  5.9366e-02,  7.4685e-02, -6.6880e-02,  7.6049e-02,\n",
      "         -5.3841e-02,  3.7525e-02, -2.4051e-02, -6.0837e-02, -4.9455e-02,\n",
      "         -1.2835e-01,  7.1273e-02,  8.2448e-03, -2.4222e-02, -2.0194e-02,\n",
      "          1.4828e-02, -8.4307e-04, -9.3173e-02, -8.8207e-02, -2.7742e-02,\n",
      "         -7.9302e-02,  7.2562e-02, -6.1378e-02, -4.1767e-02, -3.8987e-02,\n",
      "         -2.9905e-02,  1.2688e-02,  3.7553e-03, -1.1192e-02, -1.2206e-01,\n",
      "         -3.2982e-02, -7.6435e-02,  6.5688e-02,  2.6813e-02, -1.1179e-02,\n",
      "         -4.9633e-02,  6.1542e-02,  3.0834e-02, -1.0269e-02,  6.4870e-02,\n",
      "         -5.7638e-02,  4.3060e-03, -1.0008e-01, -3.5091e-01,  6.5391e-02,\n",
      "          1.0947e-02,  3.4814e-02,  3.4923e-02, -9.1231e-02,  3.7350e-02,\n",
      "          2.6930e-02,  5.2713e-05,  8.5150e-02, -6.1180e-02,  2.9461e-02,\n",
      "         -3.6984e-02, -8.5079e-02,  2.6530e-02, -3.3637e-02, -5.4362e-02,\n",
      "          5.5838e-02, -3.9802e-02, -2.1221e-02, -1.4457e-01,  1.7748e-02,\n",
      "         -4.1487e-02, -1.0857e-02,  9.1439e-03, -2.6833e-03, -6.3365e-02,\n",
      "         -7.0894e-02,  2.3556e-02,  8.8968e-02,  2.6706e-02, -1.0208e-01,\n",
      "         -5.1768e-02,  2.8664e-02,  6.1897e-02,  1.1344e-01, -5.2443e-02,\n",
      "         -3.8825e-03, -9.8364e-02,  8.7298e-02,  8.0411e-02,  2.0813e-01,\n",
      "         -3.0991e-02,  1.8852e-01,  8.7562e-03,  1.1540e-01,  6.2689e-02,\n",
      "         -2.1062e-02, -8.3236e-02, -4.2962e-02, -3.3596e-02, -4.5224e-02,\n",
      "          4.4709e-02, -6.4868e-02, -4.5981e-02,  4.6211e-02,  3.0328e-03,\n",
      "         -6.5393e-03, -1.5591e-02,  1.8842e-01,  2.3078e-03,  7.2521e-03,\n",
      "         -2.7918e-02, -1.3538e-02, -6.2095e-03,  8.3305e-05, -5.9651e-03,\n",
      "         -3.7315e-02,  1.7645e-02, -5.0366e-02, -4.9478e-03, -5.8123e-02,\n",
      "         -1.8941e-02, -1.6458e-02,  2.2910e-02,  1.1502e-01, -2.0251e-02,\n",
      "          3.1692e-02, -3.7045e-02,  1.0548e-01,  3.3315e-02, -2.5456e-02,\n",
      "          5.1958e-02,  1.1188e-01,  1.2742e-02,  1.0706e-02, -2.4340e-02,\n",
      "         -2.9807e-02,  1.5678e-02,  1.3007e-01, -4.3743e-02,  1.0692e-01,\n",
      "          8.7125e-02,  5.6808e-03, -9.4614e-02,  4.2949e-02,  1.5396e-02,\n",
      "          5.0958e-02, -6.4023e-01, -3.6584e-02,  1.3790e-01, -5.4191e-03,\n",
      "         -2.2834e-02,  6.8948e-02,  3.4192e-02, -8.6519e-02, -9.7056e-03,\n",
      "         -4.6012e-02,  7.0841e-02,  5.2503e-02,  6.9356e-02, -5.9406e-02,\n",
      "         -2.7379e-02,  8.0969e-02, -5.4704e-02, -1.7284e-03,  5.3504e-02,\n",
      "         -2.6389e-01, -4.4178e-03, -5.2920e-02,  1.0923e-01, -5.2700e-02,\n",
      "          9.0411e-02,  8.1851e-02, -7.8554e-02,  3.2444e-02,  4.9755e-02,\n",
      "          4.3072e-02,  5.0764e-02,  3.7505e-02, -6.3760e-02,  5.7162e-02,\n",
      "          1.1650e-01,  7.0077e-02,  4.1648e-02,  1.1792e+01,  2.6194e-02,\n",
      "          5.4933e-02,  3.7338e-03,  5.0105e-02, -6.1551e-02,  4.3107e-03,\n",
      "         -1.3176e-01, -1.2037e-02,  1.6352e-01, -2.9754e-02, -6.3834e-02,\n",
      "         -5.8230e-02, -1.0544e-01,  3.1869e-02,  6.2311e-03, -4.2405e-02,\n",
      "         -4.6961e-02,  5.2618e-02, -8.9570e-02, -5.9202e-02,  1.1712e-02,\n",
      "          8.3972e-02,  2.3883e-02, -2.3082e-02,  2.9189e-02,  7.6207e-02,\n",
      "         -1.0634e-01, -2.9694e-02,  7.7876e-02,  9.6785e-03,  1.1147e-02,\n",
      "          4.7138e-02,  1.6069e-02,  1.3129e-01,  3.6985e-02, -1.9742e-02,\n",
      "          6.0056e-02, -1.1609e-02,  3.4008e-02,  3.5811e-02, -1.9208e-02,\n",
      "          1.0374e-01,  3.6435e-02,  4.5250e-02,  5.9725e-03,  4.0341e-03,\n",
      "          1.5096e-01,  4.5603e-02,  6.4334e-02,  1.1201e-01, -4.6618e-02,\n",
      "          1.3388e-01,  2.9168e-02,  2.5302e-03,  2.5713e-02,  2.9211e-02,\n",
      "         -1.4521e-02,  9.1087e-02,  5.0253e-02, -3.9415e-02,  6.7097e-02,\n",
      "         -6.5439e-03,  3.5600e-02, -1.2213e-02,  1.0045e-01,  1.3769e-01,\n",
      "          6.0418e-02, -1.0507e-01, -6.9069e-02, -6.7938e-03, -5.1517e-02,\n",
      "         -4.5078e-02, -1.1527e-02,  8.1811e-02, -5.7189e-02, -4.4112e-03,\n",
      "         -3.5223e-02, -1.0305e-02, -2.2172e-02, -1.6422e-02,  1.5389e-02,\n",
      "         -2.1827e-03, -4.5947e-02,  8.8067e-02,  5.5791e-02,  2.7615e-02,\n",
      "          7.2094e-02, -4.6943e-02, -1.1526e-01, -9.8929e-03,  5.1136e-02,\n",
      "         -3.5075e-02, -3.8651e-02,  5.1887e-02, -9.0843e-02,  1.0622e-01,\n",
      "         -1.4955e-01,  3.3512e-02,  1.0368e-01, -1.0890e-01,  2.4778e-02,\n",
      "         -2.4435e-02,  6.0282e-02,  2.6010e-02,  3.5580e-02, -4.4499e-02,\n",
      "         -1.6905e-02,  3.7836e-02, -1.1840e-02, -3.5667e-02,  1.0957e-02,\n",
      "          4.4288e-02, -5.1330e-02,  4.1176e-02,  3.1323e-02, -7.8614e-03,\n",
      "          2.0006e-02,  2.4279e-02,  8.9774e-02, -7.5258e-02, -2.9248e-02,\n",
      "         -6.0289e-02, -1.6029e-02, -4.1149e-02, -2.1787e-02, -1.8895e-02,\n",
      "          6.4288e-02, -3.0288e-02,  6.4764e-02, -8.8052e-03,  1.9616e-02,\n",
      "          7.1916e-02, -7.3103e-02,  7.4203e-02, -6.2869e-02, -4.3141e-02,\n",
      "          4.4882e-02, -1.1754e-02,  6.3197e-02, -1.8505e-02, -9.1761e-02,\n",
      "         -5.1052e-02, -9.6919e-02,  1.3797e-02,  8.5993e-02,  6.6709e-02,\n",
      "          7.3238e-02,  6.6428e-02, -8.6125e-02, -7.8206e-02,  4.3027e-02,\n",
      "          6.9463e-02,  6.3709e-03,  1.1772e-02, -5.7913e-02, -5.3001e-02,\n",
      "          1.0727e-01, -5.5523e-03, -3.1501e-02,  2.5130e-02,  4.9741e-02,\n",
      "          9.8048e-02,  3.5870e-02,  9.9608e-02,  2.4971e-02,  1.6551e-02,\n",
      "          1.3239e-02,  3.8363e-02, -4.5335e-02,  3.9748e-02,  1.3632e-02,\n",
      "         -1.1738e-01, -7.9431e-02,  4.5755e-02,  9.3624e-02,  7.1120e-03,\n",
      "         -1.1123e-01, -1.1741e-02, -3.8916e-02]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "try_model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the input text\n",
    "text = \"This is an example sentence.\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Feed the input into the model\n",
    "output = try_model(**encoded_input)\n",
    "\n",
    "# Extract the CLS token embedding\n",
    "cls_embedding = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(cls_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "# model(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer, RobertaModelWithHeads\n",
    "# from transformers.adapters.composition import Stack\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "# model_name = \"roberta-base\"\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "# model = RobertaModelWithHeads.from_pretrained(model_name)\n",
    "\n",
    "# # Add a new adapter\n",
    "# adapter_name = \"my_adapter\"\n",
    "# model.add_adapter(adapter_name)\n",
    "\n",
    "# # Add a classification head for your task\n",
    "# num_labels = 2  # Change this value according to your task\n",
    "# model.add_classification_head(adapter_name, num_labels=num_labels)\n",
    "\n",
    "# # Set the adapters to be used in training\n",
    "# model.train_adapter([adapter_name])\n",
    "\n",
    "# # You can now train your model with your dataset and use it for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This movie was amazing!\"\n",
    "# cls = model(text)\n",
    "# print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from load_wiki_dataset import wikiData\n",
    "from RoBERTa import CustomRobertaModel\n",
    "from losses import align_loss, uniform_loss\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')\n",
    "    parser.add_argument('--data_path', type=str, default='../data/wiki1m_for_simcse.txt', help='Path to the dataset')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def train_wiki(args,model):\n",
    "    # Load dataset\n",
    "    with open(args.data_path, 'r', encoding='UTF-8') as f:\n",
    "        input_text = f.readlines()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # ADD THIS LINE\n",
    "\n",
    "    wiki = wikiData(input_text, tokenizer)  # PASS tokenizer TO wikiData\n",
    "    train_params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    trainloader = DataLoader(wiki, **train_params)\n",
    "    \n",
    "    # Initialize model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    # Training loop\n",
    "    # for epoch in range(args.epochs):\n",
    "    #     epoch_loss = 0\n",
    "    #     for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "    #         optimizer.zero_grad()\n",
    "    #         batch=tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    #         batch = [torch.tensor(item).to(device) for item in batch]  # Move batch to device (GPU), change it according to dataset\n",
    "    #         loss, _ = model(batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         epoch_loss += loss.item()\n",
    "\n",
    "    #     print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    for epoch in range(args.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}  # move batch to device\n",
    "                \n",
    "                loss, _ = model(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                # print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1:   0%|          | 2/125000 [00:00<6:35:40,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -5.239096641540528e-06\n",
      "Epoch 1 Loss: -1.0496516704559326e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4/125000 [00:00<6:04:56,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -1.6157252311706543e-05\n",
      "Epoch 1 Loss: -2.1675745010375977e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 6/125000 [00:01<5:57:09,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -2.786326026916504e-05\n",
      "Epoch 1 Loss: -3.3905963897705076e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 8/125000 [00:01<5:52:06,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.977145576477051e-05\n",
      "Epoch 1 Loss: -4.627012300491333e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 10/125000 [00:01<5:48:45,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -5.293617868423462e-05\n",
      "Epoch 1 Loss: -5.910845804214478e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 12/125000 [00:02<5:47:37,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -6.586467790603638e-05\n",
      "Epoch 1 Loss: -7.295846033096313e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 14/125000 [00:02<5:46:39,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -8.022289848327636e-05\n",
      "Epoch 1 Loss: -8.72768268585205e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 16/125000 [00:02<5:46:59,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -9.504745817184449e-05\n",
      "Epoch 1 Loss: -0.00010225456142425537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 18/125000 [00:03<5:46:57,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00010927201461791992\n",
      "Epoch 1 Loss: -0.0001166543378829956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 20/125000 [00:03<5:46:57,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00012536408710479738\n",
      "Epoch 1 Loss: -0.00013390872859954834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 22/125000 [00:03<5:50:28,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00014264993667602538\n",
      "Epoch 1 Loss: -0.00015117854499816896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 24/125000 [00:04<6:01:05,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00015948199653625488\n",
      "Epoch 1 Loss: -0.00016860245323181152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 26/125000 [00:04<5:58:10,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00017817403411865234\n",
      "Epoch 1 Loss: -0.00018832558155059813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 28/125000 [00:04<5:52:09,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.000199801944732666\n",
      "Epoch 1 Loss: -0.00021113076496124267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 30/125000 [00:05<5:49:02,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00022201366233825685\n",
      "Epoch 1 Loss: -0.00023444610977172852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 32/125000 [00:05<5:47:58,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0002467557430267334\n",
      "Epoch 1 Loss: -0.00025791843509674075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 34/125000 [00:05<5:54:56,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0002707911710739136\n",
      "Epoch 1 Loss: -0.0002848299674987793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 36/125000 [00:06<6:05:02,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0002984410343170166\n",
      "Epoch 1 Loss: -0.00031246599769592284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 38/125000 [00:06<6:01:30,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00032730472087860106\n",
      "Epoch 1 Loss: -0.00034177399253845217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 40/125000 [00:06<6:01:06,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0003561862735748291\n",
      "Epoch 1 Loss: -0.0003709569721221924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 42/125000 [00:07<5:57:51,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00038586863708496094\n",
      "Epoch 1 Loss: -0.00040172272300720215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 44/125000 [00:07<5:57:20,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0004168597707748413\n",
      "Epoch 1 Loss: -0.0004321925363540649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 46/125000 [00:07<5:52:52,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0004482038927078247\n",
      "Epoch 1 Loss: -0.0004648359956741333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 48/125000 [00:08<5:50:26,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.00048119721126556396\n",
      "Epoch 1 Loss: -0.0004971497087478638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 50/125000 [00:08<5:48:07,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0005147944898605347\n",
      "Epoch 1 Loss: -0.0005312362852096557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 52/125000 [00:08<5:49:12,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0005492385358810425\n",
      "Epoch 1 Loss: -0.0005675400762557984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 54/125000 [00:09<5:48:28,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0005852455015182495\n",
      "Epoch 1 Loss: -0.0006032586908340454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 56/125000 [00:09<5:48:12,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0006215950746536255\n",
      "Epoch 1 Loss: -0.0006407847185134888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 58/125000 [00:09<5:47:19,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0006594830503463745\n",
      "Epoch 1 Loss: -0.0006787323369979859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 60/125000 [00:10<5:47:31,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0006993551054000855\n",
      "Epoch 1 Loss: -0.0007184286680221557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 62/125000 [00:10<5:47:23,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0007381476087570191\n",
      "Epoch 1 Loss: -0.0007583248128890991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 64/125000 [00:10<5:47:42,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0007783242959976196\n",
      "Epoch 1 Loss: -0.000797562352180481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 66/125000 [00:11<5:46:48,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0008175622911453247\n",
      "Epoch 1 Loss: -0.0008386921396255493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 68/125000 [00:11<5:47:14,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0008595007925033569\n",
      "Epoch 1 Loss: -0.0008800246305465698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 70/125000 [00:11<5:47:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0009012965459823609\n",
      "Epoch 1 Loss: -0.0009229916505813599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 72/125000 [00:12<5:46:53,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0009451745309829711\n",
      "Epoch 1 Loss: -0.0009658331117630005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 73/125000 [00:12<5:55:16,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.0009881509828567506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_model \u001b[39m=\u001b[39m train_model(args)\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Save the trained model if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m trained_model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m./trained_model_wiki\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     59\u001b[0m loss, _ \u001b[39m=\u001b[39m model(batch)\n\u001b[0;32m     60\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 61\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     62\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(trainloader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\optim\\adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[0;32m    163\u001b[0m           grads,\n\u001b[0;32m    164\u001b[0m           exp_avgs,\n\u001b[0;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[0;32m    167\u001b[0m           state_steps,\n\u001b[0;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\optim\\adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 219\u001b[0m func(params,\n\u001b[0;32m    220\u001b[0m      grads,\n\u001b[0;32m    221\u001b[0m      exp_avgs,\n\u001b[0;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    224\u001b[0m      state_steps,\n\u001b[0;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\torch\\optim\\adamw.py:274\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    273\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 274\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[0;32m    277\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_wiki(args)\n",
    "    # Save the trained model if needed\n",
    "trained_model.save_pretrained('./trained_model_wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RoBERTa import CustomRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_model2 = CustomRobertaModel()\n",
    "wiki_model2.load_state_dict(torch.load('1wiki.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./trained_model_wiki were not used when initializing RobertaModel: ['roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.value.bias', 'roberta.roberta.encoder.layer.4.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.attention.self.query.weight', 'roberta.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.output.dense.weight', 'roberta.roberta.encoder.layer.8.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.roberta.encoder.layer.11.output.dense.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.key.weight', 'roberta.roberta.encoder.layer.1.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.attention.self.key.bias', 'roberta.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.query.weight', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.7.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.attention.self.query.weight', 'roberta.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.9.output.dense.weight', 'roberta.roberta.encoder.layer.9.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.5.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.4.attention.self.query.bias', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.10.attention.self.query.weight', 'roberta.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.8.attention.self.value.bias', 'roberta.roberta.encoder.layer.8.attention.output.dense.weight', 'roberta_m.roberta.embeddings.position_embeddings.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.query.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.query.bias', 'roberta.roberta.encoder.layer.11.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.2.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.roberta.embeddings.LayerNorm.weight', 'roberta.roberta.encoder.layer.8.attention.self.query.bias', 'roberta.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.2.output.dense.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.attention.self.key.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.3.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.embeddings.token_type_embeddings.weight', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.11.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.3.output.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.1.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.8.output.dense.weight', 'roberta_m.roberta.encoder.layer.11.attention.self.value.bias', 'roberta.roberta.encoder.layer.3.attention.self.query.bias', 'roberta.roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.roberta.encoder.layer.3.output.dense.bias', 'roberta.roberta.encoder.layer.11.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.7.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.key.weight', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.output.dense.bias', 'roberta_m.roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.value.weight', 'roberta.roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.5.attention.self.value.weight', 'roberta.roberta.encoder.layer.6.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.10.output.dense.weight', 'roberta_m.roberta.encoder.layer.7.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.value.bias', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.value.weight', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.query.bias', 'roberta.roberta.encoder.layer.5.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.value.weight', 'roberta.roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.intermediate.dense.weight', 'roberta_m.roberta.embeddings.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.output.dense.weight', 'roberta.roberta.encoder.layer.3.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.0.attention.self.key.weight', 'roberta.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.attention.self.key.weight', 'roberta.roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.roberta.encoder.layer.6.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.4.attention.self.key.bias', 'roberta.roberta.encoder.layer.2.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.0.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.attention.self.query.bias', 'roberta.roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.roberta.encoder.layer.3.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.6.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.value.weight', 'roberta.roberta.encoder.layer.0.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.attention.self.value.bias', 'roberta.roberta.encoder.layer.1.attention.self.key.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.key.weight', 'roberta.roberta.encoder.layer.2.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.roberta.pooler.dense.bias', 'roberta_m.roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.roberta.encoder.layer.11.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.0.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.key.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.attention.self.query.weight', 'roberta.roberta.embeddings.token_type_embeddings.weight', 'roberta.roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.8.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.9.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.value.bias', 'roberta.roberta.pooler.dense.weight', 'roberta.roberta.encoder.layer.2.attention.self.query.weight', 'roberta.roberta.encoder.layer.6.output.dense.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.8.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.roberta.encoder.layer.10.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.11.output.dense.weight', 'roberta.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.query.weight', 'roberta.roberta.encoder.layer.7.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.2.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.6.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.3.attention.self.value.weight', 'roberta.roberta.encoder.layer.5.attention.self.value.bias', 'roberta.roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.roberta.encoder.layer.11.attention.self.value.weight', 'roberta.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta_m.roberta.embeddings.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.roberta.encoder.layer.10.output.dense.bias', 'roberta.roberta.encoder.layer.2.output.dense.bias', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.10.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.5.attention.self.value.bias', 'roberta.roberta.encoder.layer.10.attention.self.key.bias', 'roberta.roberta.encoder.layer.10.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.2.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.attention.self.key.weight', 'roberta.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.output.dense.bias', 'roberta.roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.roberta.encoder.layer.3.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.3.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.5.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.5.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.5.output.dense.bias', 'roberta.roberta.encoder.layer.6.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.key.bias', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.8.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.0.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.1.output.dense.bias', 'roberta.roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.roberta.encoder.layer.7.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.6.attention.self.value.bias', 'roberta.roberta.encoder.layer.6.attention.self.query.weight', 'roberta.roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.roberta.encoder.layer.4.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.0.attention.self.query.weight', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.6.output.dense.weight', 'roberta.roberta.encoder.layer.7.attention.self.value.bias', 'roberta.roberta.encoder.layer.0.attention.self.key.weight', 'roberta.roberta.encoder.layer.0.attention.self.query.bias', 'roberta.roberta.encoder.layer.6.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.dense.bias', 'roberta_m.roberta.pooler.dense.weight', 'roberta.roberta.encoder.layer.11.attention.self.key.bias', 'roberta.roberta.encoder.layer.2.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.key.weight', 'roberta.roberta.encoder.layer.5.output.dense.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.value.bias', 'roberta.roberta.encoder.layer.1.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.1.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.4.output.dense.weight', 'roberta.roberta.encoder.layer.8.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.8.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.output.dense.weight', 'roberta_m.roberta.encoder.layer.6.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.key.bias', 'roberta.roberta.embeddings.position_ids', 'roberta_m.roberta.encoder.layer.11.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.query.weight', 'roberta.roberta.encoder.layer.1.attention.output.dense.weight', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.9.attention.self.value.bias', 'roberta.roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.attention.self.query.bias', 'roberta.roberta.encoder.layer.9.output.dense.bias', 'roberta_m.roberta.encoder.layer.0.output.dense.bias', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.roberta.encoder.layer.9.attention.self.value.weight', 'roberta.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.4.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.self.query.bias', 'roberta_m.roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.roberta.embeddings.word_embeddings.weight', 'roberta_m.roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.3.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.value.weight', 'roberta.roberta.embeddings.LayerNorm.bias', 'roberta.roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.8.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.7.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.11.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.roberta.encoder.layer.7.output.dense.bias', 'roberta_m.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.7.attention.self.key.bias', 'roberta.roberta.encoder.layer.0.output.dense.weight', 'roberta_m.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.attention.self.value.bias', 'roberta.roberta.encoder.layer.4.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.roberta.encoder.layer.6.attention.self.key.weight', 'roberta.roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.roberta.encoder.layer.0.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.4.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.roberta.encoder.layer.4.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.6.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.embeddings.word_embeddings.weight', 'roberta_m.roberta.encoder.layer.2.attention.self.key.bias', 'roberta.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.10.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.1.output.dense.weight', 'roberta_m.roberta.encoder.layer.10.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.4.output.dense.bias', 'roberta.roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta_m.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.0.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.key.weight', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias', 'roberta.roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.4.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.value.weight', 'roberta_m.roberta.encoder.layer.1.attention.self.query.bias', 'roberta.roberta.encoder.layer.3.attention.self.key.bias', 'roberta.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias', 'roberta_m.roberta.encoder.layer.3.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.2.output.dense.bias', 'roberta.roberta.encoder.layer.10.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.3.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.3.attention.self.key.bias', 'roberta_m.roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.roberta.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight', 'roberta.roberta.encoder.layer.1.output.dense.weight', 'roberta_m.roberta.embeddings.position_ids', 'roberta_m.roberta.encoder.layer.10.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.9.attention.self.query.bias', 'roberta.roberta.encoder.layer.1.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.roberta.encoder.layer.2.attention.self.key.bias', 'roberta.roberta.embeddings.position_embeddings.weight', 'roberta_m.roberta.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.4.attention.self.key.weight', 'roberta_m.roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.roberta.encoder.layer.7.attention.self.value.weight', 'roberta.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.1.intermediate.dense.weight', 'roberta_m.roberta.encoder.layer.1.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.6.output.dense.bias', 'roberta.roberta.encoder.layer.6.attention.self.value.bias', 'roberta.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias', 'roberta.roberta.encoder.layer.0.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.1.intermediate.dense.weight', 'roberta_m.roberta.pooler.dense.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.query.bias', 'roberta.roberta.encoder.layer.8.output.dense.bias', 'roberta_m.roberta.encoder.layer.7.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.value.bias', 'roberta.roberta.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight', 'roberta_m.roberta.encoder.layer.8.attention.self.query.weight', 'roberta.roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.2.attention.self.query.weight', 'roberta_m.roberta.encoder.layer.5.intermediate.dense.bias', 'roberta_m.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta_m.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.10.attention.self.value.weight', 'roberta.roberta.encoder.layer.5.attention.self.query.weight', 'roberta.roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.roberta.encoder.layer.0.output.dense.bias', 'roberta.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.roberta.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.roberta.encoder.layer.4.attention.self.value.bias', 'roberta_m.roberta.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias', 'roberta_m.roberta.encoder.layer.10.attention.self.query.weight', 'roberta.roberta.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight', 'roberta.roberta.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight', 'roberta_m.roberta.encoder.layer.11.attention.output.dense.bias', 'roberta_m.roberta.encoder.layer.5.attention.self.query.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./trained_model_wiki and are newly initialized: ['roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,RobertaTokenizer # Load the trained model \n",
    "\n",
    "wiki_model = AutoModel.from_pretrained('./trained_model_wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_allsides_dataset import allsidesData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import argparse\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from load_wiki_dataset import wikiData\n",
    "from losses import align_loss, uniform_loss\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "def train_allsides(args, model_to_train):\n",
    "    # Load dataset\n",
    "    with open(args.data_path, 'r', encoding='UTF-8') as f:\n",
    "        input_text = f.readlines()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # ADD THIS LINE\n",
    "\n",
    "    allsides = allsidesData(input_text, tokenizer)  # PASS tokenizer TO wikiData\n",
    "    train_params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    trainloader = DataLoader(allsides, **train_params)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = model_to_train\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    # Training loop\n",
    "    # for epoch in range(args.epochs):\n",
    "    #     epoch_loss = 0\n",
    "    #     for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "    #         optimizer.zero_grad()\n",
    "    #         batch=tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    #         batch = [torch.tensor(item).to(device) for item in batch]  # Move batch to device (GPU), change it according to dataset\n",
    "    #         loss, _ = model(batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         epoch_loss += loss.item()\n",
    "\n",
    "    #     print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    for epoch in range(args.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{args.epochs}\"):\n",
    "                optimizer.zero_grad()\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}  # move batch to device\n",
    "                # print(model(batch))\n",
    "                loss, cls = model(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                # print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/allsides.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|| 65720/65720 [3:01:13<00:00,  6.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -3.609909778521399\n"
     ]
    }
   ],
   "source": [
    "allsides_wiki_model = train_allsides(args,wiki_model2)\n",
    "    # Save the trained model if needed\n",
    "allsides_wiki_model.save_pretrained('./allsides_wiki_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(allsides_wiki_model.state_dict(), 'allsides_wiki_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.epochs = 1\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 1e-4\n",
    "args.data_path = \"../data/wiki1m_for_simcse.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  70%|   | 86938/125000 [3:55:10<1:42:57,  6.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m allsides_wiki2_model \u001b[39m=\u001b[39m train_wiki(args,allsides_wiki_model)\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Save the trained model if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m allsides_wiki2_model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m./allsides_wiki2_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 61\u001b[0m, in \u001b[0;36mtrain_wiki\u001b[1;34m(args, model)\u001b[0m\n\u001b[0;32m     59\u001b[0m             loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 61\u001b[0m             epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     62\u001b[0m             \u001b[39m# print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(trainloader)}\")\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(trainloader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "allsides_wiki2_model = train_wiki(args,allsides_wiki_model)\n",
    "    # Save the trained model if needed\n",
    "allsides_wiki2_model.save_pretrained('./allsides_wiki2_model')\n",
    "torch.save(allsides_wiki2_model.state_dict(), 'allsides_wiki2_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Ben\\anaconda3\\envs\\545\\lib\\site-packages\\transformers\\adapters\\models\\roberta\\adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RoBERTa import CustomRobertaModel\n",
    "wiki_model = CustomRobertaModel()\n",
    "wiki_model.load_state_dict(torch.load('1wiki.pth'))\n",
    "allsides_wiki_model = CustomRobertaModel()\n",
    "allsides_wiki_model.load_state_dict(torch.load('allsides_wiki_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdf77093bbdbf1309a6eb466c68765f3f88bd63ff87e3bfe863ca62958fcd8bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
