{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfa2795a125427b9330b69700ec5f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from RoBERTa import *\n",
    "model = CustomRobertaModel(adapter_name=\"AdapterHub/bert-base-uncased-pf-imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRobertaModel(\n",
       "  (shared_parameters): ModuleDict()\n",
       "  (invertible_adapters): ModuleDict()\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (key): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (value): Linear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (prefix_tuning): PrefixTuningShim(\n",
       "              (prefix_gates): ModuleDict()\n",
       "              (pool): PrefixTuningPool(\n",
       "                (prefix_tunings): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (prefix_tuning): PrefixTuningPool(\n",
       "    (prefix_tunings): ModuleDict()\n",
       "  )\n",
       "  (roberta): RobertaModel(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (imdb): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.8915e-02,  4.8113e-02, -2.7286e-02, -7.5899e-02,  6.4266e-02,\n",
      "         -7.3705e-02, -2.2010e-02,  2.7606e-02,  6.9526e-02, -6.2947e-02,\n",
      "         -1.4534e-02,  2.2958e-02,  4.3499e-02, -1.5362e-02,  8.6064e-02,\n",
      "          1.3907e-02, -1.1446e-01, -8.8599e-03,  3.4619e-02,  1.1851e-02,\n",
      "         -1.0749e-01,  3.6599e-02, -2.5159e-02,  1.1196e-01,  4.0717e-02,\n",
      "          1.1286e-02,  4.2195e-02,  9.1134e-02, -6.5316e-02, -2.9515e-02,\n",
      "         -4.1899e-02, -2.8931e-02,  1.1868e-02, -4.3952e-02,  2.9498e-02,\n",
      "          1.3717e-01,  5.2524e-02, -4.4852e-02, -1.2761e-01, -4.5893e-03,\n",
      "         -3.5143e-02,  5.0591e-02,  7.0719e-04, -4.3527e-03,  5.6598e-02,\n",
      "         -1.3464e-03,  9.6438e-03,  2.3344e-03, -7.1856e-02,  2.4527e-02,\n",
      "         -2.2287e-04,  7.7735e-02, -6.0245e-03,  5.5217e-03, -6.5835e-02,\n",
      "          1.1370e-02, -3.0623e-02,  9.3616e-02,  4.7119e-02, -8.4259e-03,\n",
      "          1.0089e-02, -1.0109e-01, -1.1778e-01, -3.5337e-02,  4.5025e-03,\n",
      "         -1.4769e-02, -3.2204e-02,  2.4516e-02,  2.1691e-02,  4.7243e-02,\n",
      "          8.9386e-02, -2.3828e-02,  3.5249e-02, -2.6674e-02, -6.7181e-03,\n",
      "          2.4274e-02, -1.9790e-02,  5.0309e-01, -3.4767e-02, -2.3299e-03,\n",
      "          5.4681e-02, -1.7812e-02,  3.9130e-01,  3.4564e-02, -2.5973e-02,\n",
      "         -3.1066e-02,  8.3521e-02,  4.0689e-02, -6.8652e-03,  2.0569e-04,\n",
      "          1.1973e-02,  4.1409e-02, -4.7509e-02,  1.1827e-02,  3.0729e-02,\n",
      "          7.4804e-02,  1.3083e-02,  1.5020e-02, -3.8044e-02, -4.0267e-02,\n",
      "         -2.9753e-02, -8.0078e-02,  1.2129e-01,  4.7329e-02,  4.1354e-03,\n",
      "          2.3807e-02, -6.3491e-04, -2.2187e-02, -1.1085e-02, -4.8663e-02,\n",
      "          1.0300e-02,  2.6783e-02, -2.4668e-02,  6.3816e-02, -4.6697e-02,\n",
      "         -5.2706e-02, -6.9412e-03,  2.8186e-02,  1.3256e-02,  1.7580e-02,\n",
      "          1.4232e-02,  9.9485e-02,  1.3243e-01,  9.7089e-03, -4.7512e-03,\n",
      "         -4.2787e-03,  4.8027e-02,  1.3219e-02,  9.1840e-03,  1.4371e-02,\n",
      "         -1.7198e-03, -7.8931e-02, -5.5065e-03,  1.1544e-01,  5.8994e-02,\n",
      "          3.3821e-02,  5.9104e-02, -3.5006e-02,  7.3672e-02,  2.7062e-03,\n",
      "         -4.1141e-02,  3.2895e-02,  6.7777e-02,  2.9227e-02,  1.0960e-01,\n",
      "          3.6326e-02,  9.9583e-03, -3.3355e-02,  5.2718e-02, -1.3920e-02,\n",
      "          2.2765e-02, -7.1550e-02, -6.2461e-02,  5.6348e-02,  4.3285e-02,\n",
      "          3.6004e-01,  7.4687e-02,  2.9604e-02, -4.4167e-02,  1.3336e-02,\n",
      "          1.3379e-01, -9.7049e-03, -8.4647e-03,  1.9590e-03, -3.2808e-02,\n",
      "          2.4424e-02,  3.0148e-02,  3.4987e-02,  3.8388e-02, -1.1494e-02,\n",
      "          5.3879e-02,  2.1566e-03,  2.3513e-02, -7.3864e-02, -3.3981e-02,\n",
      "         -8.9099e-02, -2.9776e-02,  1.2392e-02, -1.0517e-01,  7.9469e-03,\n",
      "          6.9304e-02,  7.1530e-02, -6.8136e-02,  4.0712e-02, -3.5706e-02,\n",
      "         -8.3171e-03, -2.8863e-02,  3.4566e-02, -1.7902e-02, -6.9611e-03,\n",
      "          5.7744e-02, -5.8555e-02, -5.6019e-03,  1.8733e-02,  1.1133e-02,\n",
      "          1.1165e-01, -1.9754e-02, -2.9267e-02,  8.6740e-03, -5.0423e-02,\n",
      "          4.9226e-02, -9.7565e-02,  9.2208e-02, -5.1182e-02,  5.7344e-02,\n",
      "          4.5925e-03,  5.8453e-02,  9.8696e-02,  2.4436e-02, -5.7253e-02,\n",
      "         -6.2437e-02,  8.5977e-02,  2.0695e-02,  8.5414e-02,  2.3906e-02,\n",
      "         -1.6168e-02,  5.8143e-02,  1.0644e-01, -1.5632e-02, -3.0417e-02,\n",
      "          4.6960e-02,  4.2323e-02, -1.3743e-02,  8.0939e-02, -1.9509e-02,\n",
      "          2.0072e-02,  1.6207e-02,  1.3682e-02,  2.9977e-02, -3.6064e-02,\n",
      "         -2.9665e-03,  2.9152e-02,  1.3197e-02, -2.9720e-02, -1.2650e-02,\n",
      "         -1.7247e-01,  7.6007e-03, -2.8316e-02, -4.6901e-02,  3.4326e-02,\n",
      "         -1.2890e-01,  3.5735e-02,  2.7673e-02, -2.1847e-02,  1.5911e-02,\n",
      "          7.0001e-02, -1.0012e-02,  8.7260e-02, -3.8209e-03,  3.1100e-02,\n",
      "         -3.3074e-03,  2.1994e-02, -3.5033e-02, -3.0869e-02,  3.5271e-02,\n",
      "         -2.4892e-02, -1.1613e-01, -9.7935e-04,  2.3802e-02,  2.9884e-02,\n",
      "          2.8605e-03, -9.1504e-02,  3.2875e-02, -4.1024e-02, -9.4302e-02,\n",
      "         -4.7248e-02,  2.3103e-02,  1.3386e-02,  8.5534e-03, -8.6719e-02,\n",
      "         -2.3743e-02, -2.5736e-02, -1.7961e-02,  7.3647e-03,  7.1402e-03,\n",
      "          3.2707e-02, -4.6881e-02, -2.1543e-03, -7.7659e-02,  1.7936e-02,\n",
      "          3.4218e-02, -1.1784e-02, -9.3412e-02, -4.2658e-02, -2.8219e-02,\n",
      "          1.7899e-02,  1.7647e-02, -3.7683e-02,  2.4947e-02,  4.5115e-02,\n",
      "          8.4887e-02,  4.7469e-02, -7.6323e-03,  3.2005e-02, -3.2106e-02,\n",
      "          2.0040e-02,  6.2764e-02,  2.6576e-02,  1.7093e-02,  2.9509e-02,\n",
      "         -3.9535e-02, -1.1723e-01, -5.7762e-03,  1.5431e-02, -1.2179e-02,\n",
      "         -4.7265e-02, -1.8919e-02, -2.8050e-02,  3.3792e-02, -2.8144e-02,\n",
      "         -7.4555e-02, -4.4466e-02, -1.0143e-01,  1.2913e-01,  3.8058e-02,\n",
      "         -6.5054e-02,  4.2104e-02,  2.5435e-02,  2.2062e-02,  3.4874e-02,\n",
      "          3.3445e-02,  1.9860e-02,  5.6777e-02,  6.6319e-03, -3.2753e-02,\n",
      "          3.8627e-04,  4.8515e-02, -6.9726e-03,  3.9331e-02,  3.5596e-01,\n",
      "         -3.0918e-01,  3.8850e-02,  7.5613e-02,  2.2577e-02,  1.0664e-01,\n",
      "          8.1912e-03,  4.6617e-02,  2.7573e-02,  5.8915e-02,  6.9459e-02,\n",
      "         -4.1878e-02,  9.1053e-02,  4.5336e-03,  9.3984e-03,  1.9086e-02,\n",
      "          4.3329e-03, -2.4327e-02,  2.4718e-02, -1.8388e-02,  2.3632e-02,\n",
      "         -1.2259e-02, -1.3130e-02,  5.1694e-02, -2.2943e-02, -2.2828e-02,\n",
      "          4.9760e-02,  2.7128e-02, -1.5576e-02, -1.0377e-02, -2.2953e-02,\n",
      "          4.2495e-02,  1.9146e-02,  1.9555e-02, -3.9818e-02,  1.1633e-01,\n",
      "         -7.8221e-02, -5.1748e-02,  1.1461e-02,  1.3804e-02,  3.1990e-03,\n",
      "          8.8355e-02,  1.0759e-02, -3.2706e-02,  2.4321e-02,  4.0682e-02,\n",
      "         -2.4659e-02,  2.8588e-02, -6.1857e-04,  7.5671e-03,  2.6939e-02,\n",
      "          3.6663e-02,  5.7922e-02, -4.3749e-02,  6.0061e-03,  8.0341e-02,\n",
      "         -3.9764e-02,  7.3980e-03, -4.5401e-04,  6.5564e-02, -2.9469e-03,\n",
      "          6.8791e-03, -6.9465e-02, -4.9786e-02, -9.7462e-02,  4.9360e-02,\n",
      "         -2.9434e-02, -6.7713e-02, -1.2922e-01, -6.1095e-02, -2.0847e-02,\n",
      "          9.4360e-03,  3.2871e-02, -8.7445e-02,  7.5685e-03,  8.2473e-03,\n",
      "          3.7702e-03,  3.7501e-02, -4.3055e-02,  2.4483e-02, -3.1363e-02,\n",
      "         -2.2025e-02,  4.7980e-02,  6.2163e-02, -2.2969e-02,  8.9891e-03,\n",
      "         -8.2031e-03,  2.3011e-02, -8.2071e-03, -1.0288e-02, -6.4210e-03,\n",
      "          7.4673e-02,  5.8266e-02,  9.1899e-03,  1.7009e-02, -5.8521e-02,\n",
      "          8.5796e-03,  7.4394e-02, -5.5414e-02, -3.2063e-02, -1.2608e-03,\n",
      "          3.3664e-03,  3.8039e-02, -4.8875e-02,  1.5461e-02, -7.4527e-03,\n",
      "         -2.8072e-02, -2.0584e-03, -8.9090e-03, -2.8939e-02, -8.0688e-02,\n",
      "         -3.4834e-02, -9.5616e-03,  7.2948e-02,  4.8684e-02, -4.3456e-02,\n",
      "          1.8481e-02,  9.6880e-02, -8.6478e-03, -1.1941e-01,  2.5626e-02,\n",
      "          4.9840e-02,  9.8757e-04, -2.1244e-02, -5.7238e-01,  5.2049e-02,\n",
      "          4.5167e-02, -6.0259e-03,  1.4833e-02, -7.1551e-02, -1.7156e-02,\n",
      "          1.4568e-02,  2.9178e-02,  3.6433e-02, -3.3117e-02,  2.3596e-02,\n",
      "         -2.6099e-02,  3.5212e-02,  3.0432e-02, -3.4648e-02, -2.7640e-02,\n",
      "          4.2909e-02,  2.0672e-03,  1.6890e-02, -6.6277e-02,  1.9206e-02,\n",
      "         -2.6977e-02, -3.4726e-02,  6.4917e-02, -7.1925e-02, -9.5299e-02,\n",
      "         -4.5766e-02,  7.4451e-02,  4.8446e-02, -1.3410e-02, -7.4264e-02,\n",
      "         -3.9887e-03, -6.7163e-03,  1.6007e-02,  8.6396e-02,  3.1673e-02,\n",
      "         -2.4400e-02, -5.1577e-02,  3.6490e-02,  3.6983e-02,  2.3386e-01,\n",
      "         -1.2064e-02,  2.1171e-01,  1.8180e-02,  1.4711e-02, -7.5992e-03,\n",
      "         -2.2246e-02, -5.0464e-03,  4.0157e-02,  1.7690e-02, -3.1377e-02,\n",
      "         -3.8362e-02, -8.1233e-02, -1.5816e-03,  9.6101e-03,  2.0884e-02,\n",
      "         -3.8735e-02,  2.0650e-02,  1.0525e-01, -3.1997e-02,  4.6857e-02,\n",
      "          2.8387e-02,  2.0284e-02, -1.4732e-02, -3.8598e-02,  4.1024e-02,\n",
      "         -5.8010e-02,  6.1884e-02, -1.3073e-02, -8.3367e-02, -7.5966e-02,\n",
      "         -6.5434e-02,  1.8817e-02,  1.5653e-02,  7.1640e-02,  2.8696e-02,\n",
      "          1.8008e-02, -8.2735e-02,  5.7278e-02,  2.2742e-02,  6.0046e-02,\n",
      "          2.3772e-02,  4.6619e-02,  1.0135e-02, -5.0578e-02, -2.7230e-02,\n",
      "         -2.0752e-02,  2.4029e-02,  1.0281e-01, -2.5225e-02,  7.0908e-02,\n",
      "          7.4068e-02, -8.3175e-04, -7.7800e-03,  1.8433e-02,  4.9593e-02,\n",
      "         -4.4173e-04, -6.4320e-01, -3.7073e-02,  4.9318e-02,  4.5562e-02,\n",
      "          4.3760e-03,  4.8852e-02,  1.3836e-02, -6.7869e-02,  5.3811e-02,\n",
      "         -6.1337e-02, -6.0006e-03,  3.0123e-02,  9.7865e-02, -4.5526e-02,\n",
      "          7.4348e-03,  1.1978e-02, -4.0105e-02, -6.9986e-02,  4.4954e-02,\n",
      "         -2.5277e-01, -8.9328e-03, -4.9547e-02,  8.3618e-02,  2.2635e-02,\n",
      "          2.4337e-02,  8.3222e-02, -5.6150e-02,  3.1869e-02,  8.8466e-02,\n",
      "          8.7845e-02,  5.8156e-02,  3.2977e-02, -1.6814e-02,  3.8393e-02,\n",
      "          2.8569e-02,  8.8727e-02,  1.3521e-02,  9.9002e+00,  2.9860e-04,\n",
      "          5.7190e-02,  2.3771e-02,  2.5200e-03, -1.7328e-03,  7.8364e-02,\n",
      "         -5.5353e-02, -5.5903e-02,  7.5112e-02,  3.4852e-02,  1.8157e-02,\n",
      "         -9.0002e-02, -1.3590e-02,  3.8862e-02,  6.1391e-02, -6.1323e-02,\n",
      "         -2.6357e-02,  6.4278e-02,  2.7325e-02,  2.0981e-03,  2.5796e-02,\n",
      "          9.5548e-03,  6.4918e-02, -7.2616e-02,  1.2296e-02,  1.1396e-02,\n",
      "         -4.1185e-02, -9.3344e-03, -2.0687e-02,  2.2987e-02, -8.0466e-03,\n",
      "          3.7632e-02,  1.5810e-02,  2.3861e-02, -1.2902e-02,  1.3087e-02,\n",
      "          5.5369e-02,  3.9702e-02,  9.6319e-02,  9.8933e-02,  6.1967e-02,\n",
      "         -5.3866e-03, -2.1592e-02,  6.0610e-02,  2.9442e-02, -1.1318e-02,\n",
      "          7.3122e-02,  5.5302e-03,  7.0867e-02,  7.4078e-02, -6.8802e-02,\n",
      "          8.8386e-02,  4.6174e-02, -1.0362e-03, -1.3829e-02,  3.3815e-02,\n",
      "         -3.0224e-03,  3.8526e-02,  2.2418e-02, -3.5857e-02,  9.3509e-02,\n",
      "         -4.0034e-02,  3.9106e-02, -6.7574e-03,  7.2288e-02,  7.1858e-02,\n",
      "          1.5060e-01, -7.8535e-02,  2.0976e-03, -4.7234e-03, -7.6276e-02,\n",
      "         -5.0421e-02,  3.2416e-02,  2.1313e-02, -8.8271e-02,  5.4528e-03,\n",
      "          1.9092e-02,  1.6918e-02,  2.9089e-02, -1.6582e-02,  2.3031e-02,\n",
      "         -1.6142e-02, -9.6110e-03,  1.0198e-01,  2.3601e-02,  5.7101e-02,\n",
      "          9.4351e-02, -1.3057e-02, -1.1064e-02, -7.7272e-02,  4.6974e-02,\n",
      "          2.5564e-03, -5.5540e-02,  4.0670e-02,  1.6923e-02, -5.2585e-03,\n",
      "         -2.6799e-02,  8.3566e-02,  6.2484e-02, -5.8593e-02,  2.8382e-02,\n",
      "          1.2501e-02,  5.3420e-02,  2.2102e-02,  2.6576e-02, -3.2057e-02,\n",
      "         -4.7151e-02,  1.2598e-02, -3.8772e-03, -4.6782e-02,  5.7554e-03,\n",
      "         -3.0745e-02, -2.4487e-02,  1.9209e-02,  2.1823e-02, -3.5403e-03,\n",
      "          4.4118e-02,  1.1874e-02,  4.3797e-02, -8.7457e-02,  1.0553e-02,\n",
      "         -5.5248e-03, -2.0380e-02,  1.3701e-02, -9.1736e-02, -2.1226e-02,\n",
      "          4.6985e-02,  5.1837e-02,  1.2686e-02, -4.8042e-02,  1.7997e-02,\n",
      "          4.9567e-02, -3.8359e-03,  6.1696e-02, -2.4787e-02,  4.8259e-03,\n",
      "          5.2751e-02,  2.4025e-02,  5.6686e-02, -2.8294e-02, -1.5386e-02,\n",
      "         -2.3640e-02, -9.0753e-02, -8.7334e-03, -7.1059e-03,  7.8661e-02,\n",
      "          2.5296e-02, -1.4603e-03, -7.8358e-03,  4.2171e-02,  2.3196e-02,\n",
      "          3.5441e-02,  2.0672e-02,  4.9072e-02,  1.0908e-02, -3.0584e-03,\n",
      "          5.2626e-02, -1.0459e-02, -2.6098e-03,  4.3884e-02, -3.0713e-02,\n",
      "          5.8091e-03,  1.8482e-02, -7.3889e-02, -3.0350e-02, -1.3753e-02,\n",
      "          4.0640e-02,  4.1812e-02, -1.7776e-02,  2.4982e-02, -1.8963e-02,\n",
      "         -4.1181e-02, -9.4397e-02,  2.1218e-02,  5.6821e-02,  2.1514e-02,\n",
      "         -3.9621e-02, -3.6391e-02,  3.4878e-02]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"This movie was amazing!\"\n",
    "cls = model(text)\n",
    "print(cls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
