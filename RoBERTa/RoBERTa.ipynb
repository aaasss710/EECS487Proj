{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685aca49c9714743b6950541090a8e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {\n",
       "      \"imdb\": \"9076f36a74755ac4\"\n",
       "    },\n",
       "    \"config_map\": {\n",
       "      \"9076f36a74755ac4\": {\n",
       "        \"adapter_residual_before_ln\": false,\n",
       "        \"cross_adapter\": false,\n",
       "        \"factorized_phm_W\": true,\n",
       "        \"factorized_phm_rule\": false,\n",
       "        \"hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
       "        \"init_weights\": \"bert\",\n",
       "        \"inv_adapter\": null,\n",
       "        \"inv_adapter_reduction_factor\": null,\n",
       "        \"is_parallel\": false,\n",
       "        \"learn_phm\": true,\n",
       "        \"leave_out\": [],\n",
       "        \"ln_after\": false,\n",
       "        \"ln_before\": false,\n",
       "        \"mh_adapter\": false,\n",
       "        \"non_linearity\": \"relu\",\n",
       "        \"original_ln_after\": true,\n",
       "        \"original_ln_before\": true,\n",
       "        \"output_adapter\": true,\n",
       "        \"phm_bias\": true,\n",
       "        \"phm_c_init\": \"normal\",\n",
       "        \"phm_dim\": 4,\n",
       "        \"phm_init_range\": 0.0001,\n",
       "        \"phm_layer\": false,\n",
       "        \"phm_rank\": 1,\n",
       "        \"reduction_factor\": 16,\n",
       "        \"residual_before_ln\": true,\n",
       "        \"scaling\": 1.0,\n",
       "        \"shared_W_phm\": false,\n",
       "        \"shared_phm_rule\": true,\n",
       "        \"use_gating\": false\n",
       "      }\n",
       "    },\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install adapter-transformers\n",
    "from transformers import RobertaModel, list_adapters\n",
    "import torch\n",
    "\n",
    "roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "adapter_name = roberta.load_adapter(\"AdapterHub/bert-base-uncased-pf-imdb\", source=\"hf\")\n",
    "roberta.active_adapters = adapter_name\n",
    "\n",
    "# Freeze all parameters except those of the adapter\n",
    "for name, param in roberta.named_parameters():\n",
    "    if f\"adapters.{adapter_name}\" not in name:\n",
    "        param.requires_grad = False\n",
    "roberta.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.0982, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "input_text = \"This is an example sentence.\"\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoded_input)\n",
    "outputs1 = roberta(**encoded_input)\n",
    "\n",
    "# Get the CLS output (the first token embedding)\n",
    "cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "cls_output1 = outputs1.last_hidden_state[:, 0, :]\n",
    "print(cls_output[0,0])\n",
    "print(cls_output1[0,0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
